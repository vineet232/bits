{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e9122c6",
   "metadata": {},
   "source": [
    "### Student information:\n",
    "- Name: Vineet Kumar\n",
    "- Roll No.: 2024AC05100\n",
    "- Assignment-1: Video Classification\n",
    "\n",
    "### Assignment objective: \n",
    "\n",
    "Using the customized dataset \"UCF-101\" to perform \"Action recognition\" for 3 different classes to predict the correct video class (Video Classification) using Classical Machine learning and Deep Learning Models.\n",
    "\n",
    "### Task: Action recognition:\n",
    "1. Using Classical Machine Learning models.\n",
    "2. Using Deep Learning Models\n",
    "\n",
    "### This python notebook file contains the code for following tasks:\n",
    "1. Loading the dataset for 3 classes:\n",
    "    - Class-1: PullUps\n",
    "    - Class-2: Punch\n",
    "    - Class-3: PullUps\n",
    "    - The dataset is split into Train, Test and Validation.\n",
    "    - For each category there is a separate CSV file.\n",
    "\n",
    "2. Demonstration of extracting the video features:\n",
    "    - For simplicity, one video from each class i.e. total 3 videos are taken.\n",
    "    - From each video 30 frames are extracted using Uniform Sampling.\n",
    "3. Feature extraction of all the videos from Train, Test and Validation dataset.\n",
    "4. Building feature matrix for Train, Test and Validation dataset.\n",
    "5. Implementation of Classical Machine Learning models like Logistic Regression, SVM etc.\n",
    "6. Model hyperparameter tuning\n",
    "7. Performance evaluation and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebbfaff",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "\n",
    "1. Upgrading pip (Optional step):\n",
    " \n",
    "    - In my PC it recommended to upgrade pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5511c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a87c5",
   "metadata": {},
   "source": [
    "1. Installing required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc66775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install(pkg):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg]);\n",
    "\n",
    "packages = [\"opencv-python\", \"scikit-image\", \"scikit-learn\", \"matplotlib\", \"numpy\", \"tqdm\"];\n",
    "\n",
    "for p in packages:\n",
    "    try:\n",
    "        __import__(p.split(\"-\")[0]);\n",
    "    except ImportError:\n",
    "        print(\"Installing package:\", p);\n",
    "        install(p);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1976f959",
   "metadata": {},
   "source": [
    "2. Importing modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b6503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import os;\n",
    "import cv2;\n",
    "import matplotlib.pyplot as plt;\n",
    "\n",
    "from tqdm import tqdm;\n",
    "from data_loader import load_dataset, extract_frames;\n",
    "from feature_extraction import video_color_features;\n",
    "from feature_extraction import video_texture_features;\n",
    "from feature_extraction import video_shape_features;\n",
    "from feature_extraction import temporal_motion_features;\n",
    "\n",
    "\n",
    "from skimage.feature import graycomatrix, graycoprops;\n",
    "from skimage.feature import local_binary_pattern;\n",
    "from skimage.filters import gabor;\n",
    "from skimage.feature import hog;\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler;\n",
    "from sklearn.pipeline import Pipeline;\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay, f1_score;\n",
    "from sklearn.model_selection import GridSearchCV;\n",
    "\n",
    "from sklearn.svm import SVC;\n",
    "from sklearn.ensemble import RandomForestClassifier;\n",
    "from sklearn.neighbors import KNeighborsClassifier;\n",
    "from sklearn.linear_model import LogisticRegression;\n",
    "from sklearn.decomposition import PCA;\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier;\n",
    "from collections import Counter;\n",
    "import time;\n",
    "import joblib;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92a80c",
   "metadata": {},
   "source": [
    "### Creating required directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd85756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(\"../dataset_info/sample_frames\", exist_ok=True);\n",
    "os.makedirs(\"../results\", exist_ok=True);\n",
    "os.makedirs(\"../results/confusion_matrices/\", exist_ok=True);\n",
    "os.makedirs(\"../results/confusion_matrices/classical\", exist_ok=True);\n",
    "os.makedirs(\"../results/feature_visualizations\", exist_ok=True);\n",
    "os.makedirs(\"../results/saved_models\", exist_ok=True);\n",
    "os.makedirs(\"../results/saved_models/classical\", exist_ok=True);\n",
    "os.makedirs(\"../results/saved_feature_matrices\", exist_ok=True);\n",
    "os.makedirs(\"../results/stats_classical\", exist_ok=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb0260e",
   "metadata": {},
   "source": [
    "### Loading the dataset and extracting video frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1dd31f",
   "metadata": {},
   "source": [
    "1. Loading and checking the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d11c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../dataset_info/dataset\";\n",
    "split_path = \"../dataset_info/dataset/splits\"\n",
    "\n",
    "train_videos, train_labels, class_map = load_dataset(f\"{split_path}/train.csv\", dataset_path)\n",
    "val_videos, val_labels, _ = load_dataset(f\"{split_path}/val.csv\", dataset_path);\n",
    "test_videos, test_labels, _ = load_dataset(f\"{split_path}/test.csv\", dataset_path);\n",
    "\n",
    "print(\"Class Mapping as per the train dataset:\\n\");\n",
    "for cls, idx in class_map.items():\n",
    "    print(\"class\", idx, \"->\", cls);\n",
    "\n",
    "print(\"\\nTotal number of videos:\", len(train_videos) + len(test_videos) + len(val_videos));\n",
    "print(\"\\nNumber of videos per class:\");\n",
    "print(\"> Number of videos in Train dataset:\", len(train_videos));\n",
    "print(\"> Number of videos in Validation dataset:\", len(val_videos));\n",
    "print(\"> Number of videos in Test dataset:\", len(test_videos));\n",
    "\n",
    "print(\"\\nTrain class distribution:\", Counter(train_labels));\n",
    "print(\"Validation class distribution:\", Counter(val_labels));\n",
    "print(\"Test class distribution:\", Counter(test_labels));\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad967c",
   "metadata": {},
   "source": [
    "2. Extracting video frames from train dataset videos\n",
    "\n",
    "-   Extracting and Displaying 30 frames of 1st video only from train dataset for simplicity.\n",
    "-   Code to extract frames for all the videos from train dataset is commented below.\n",
    "-   Please read the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d6c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#............................................................................................\n",
    "#   Extracting and Displaying 30 frames of 1st video only from train dataset for simplicity.\n",
    "#   Code to extract frames for all the videos from train dataset is commented below.\n",
    "#............................................................................................\n",
    "\n",
    "video_path = train_videos[0];\n",
    "video_name = os.path.basename(video_path);\n",
    "\n",
    "frames = extract_frames(video_path);\n",
    "\n",
    "print(\"Number of frames extracted:\", len(frames));\n",
    "if len(frames) == 0:\n",
    "    print(\"No frames extracted!\")\n",
    "else:\n",
    "    print(\"Frame Shape:\", frames[0].shape)\n",
    "\n",
    "print(\"Video Path:\", video_path);\n",
    "print(\"Video Title: \", video_name);\n",
    "\n",
    "save_dir = \"../dataset_info/sample_frames\";\n",
    "\n",
    "print(\"\\nDisplaying 30 frames of first video only for simplicity.\\n\")\n",
    "\n",
    "num_frames = len(frames)\n",
    "n_rows = 5\n",
    "n_cols = 6\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "for i in range(num_frames):\n",
    "    frame = frames[i]\n",
    "    plt.subplot(n_rows, n_cols, i + 1)\n",
    "\n",
    "    # Convert to RGB for correct visualization\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"F{i+1}\", fontsize=8)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "save_path = os.path.join(save_dir, \"sample_frames_grid.png\")\n",
    "plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(\"Sample frames saved to:\", save_path)\n",
    "\n",
    "'''\n",
    "#............................................................................................\n",
    "#  Code to extract frames for all the videos from \"train dataset\" is commented below.\n",
    "#  Remove the comments if it is required to see the extracted frames of all the videos.\n",
    "#............................................................................................\n",
    "\n",
    "\n",
    "#total_videos = 5;  (Uncomment if need to check first 5 videos only)\n",
    "\n",
    "total_videos = len(train_videos); #..... comment this line if the above line with total_videos=5 is not commented.\n",
    "\n",
    "for vid_idx in range(total_videos):\n",
    "\n",
    "    video_path = train_videos[vid_idx];\n",
    "    video_name = os.path.basename(video_path);\n",
    "    frames = extract_frames(video_path);\n",
    "\n",
    "    print(\"\\n==============================\");\n",
    "    print(\"Video\", vid_idx+1, \"/\", total_videos, \":\", video_name);\n",
    "    print(\"Total Frames:\", len(frames));\n",
    "\n",
    "    save_dir = \"../dataset_info/sample_frames\";\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True);\n",
    "\n",
    "    total_frames = len(frames);\n",
    "    n_rows = 5;\n",
    "    n_cols = 6;\n",
    "\n",
    "    plt.figure(figsize=(12, 10));\n",
    "\n",
    "    for i in range(total_frames):\n",
    "        frame = frames[i];\n",
    "        plt.subplot(n_rows, n_cols, i + 1);\n",
    "\n",
    "        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB));\n",
    "        plt.title(f\"F{i+1}\", fontsize=8);\n",
    "        plt.axis(\"off\");\n",
    "\n",
    "    plt.tight_layout();\n",
    "\n",
    "    # Save before showing\n",
    "    save_path = os.path.join(save_dir, f\"{video_name}_frames.png\");\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\");\n",
    "\n",
    "    plt.show();\n",
    "    plt.close();\n",
    "\n",
    "    print(\"Saved frames grid at:\", save_path);\n",
    "'''\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04332a17",
   "metadata": {},
   "source": [
    "### Feature Extraction (Demonstration)\n",
    "\n",
    "- In this section, 1 video from each class is taken for simplicity.\n",
    "    - Class-1: PullUps\n",
    "    - Class-2: Punch\n",
    "    - Class-3: PullUps\n",
    "- Each video consists 30 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5050dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "#  Choosing 1 video from each class....\n",
    "#-----------------------------------------\n",
    "target_classes = [\"PullUps\", \"Punch\", \"PushUps\"];\n",
    "selected_indices = {};\n",
    "\n",
    "for i in range(len(train_videos)):\n",
    "    label = train_labels[i];\n",
    "    for cls_name, cls_id in class_map.items():\n",
    "        if cls_id == label and cls_name in target_classes and cls_name not in selected_indices:\n",
    "            selected_indices[cls_name] = i;\n",
    "\n",
    "print(selected_indices);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce9182",
   "metadata": {},
   "source": [
    "Low-Level Features: \n",
    "1. Color features\n",
    "    - 1st RGB color histogram\n",
    "    - 2nd HSV color histogram\n",
    "    - 3rd Average color distribution\n",
    "    - 4th Color moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2dc612",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "#............................................... Low-Level Features............................................#\n",
    "################################################################################################################\n",
    "\n",
    "# ...............................................  Color features  ........................................... #\n",
    "\n",
    "save_dir = \"../results/feature_visualizations/color_features/RGB\";\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True);\n",
    "\n",
    "#------------------------------------------\n",
    "# 1. RGB color histograms (per frame)\n",
    "#------------------------------------------\n",
    "\n",
    "for cls_name, idx in selected_indices.items():\n",
    "\n",
    "    frames = extract_frames(train_videos[idx]);\n",
    "    num_frames = len(frames);\n",
    "\n",
    "    fig = plt.figure(figsize=(14, num_frames * 2));\n",
    "\n",
    "    for f in range(num_frames):\n",
    "        frame = frames[f];\n",
    "\n",
    "        # ---- Displaying frame on Left. ----\n",
    "        ax1 = plt.subplot(num_frames, 2, 2*f + 1);\n",
    "        ax1.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB));\n",
    "        ax1.set_title(f\"{cls_name} - F{f+1}\", fontsize=7);\n",
    "        ax1.axis(\"off\");\n",
    "\n",
    "        # ---- Displaying RGB histogram on right. ----\n",
    "        ax2 = plt.subplot(num_frames, 2, 2*f + 2);\n",
    "        colors = ('b','g','r');\n",
    "        for c, col in enumerate(colors):\n",
    "            hist = cv2.calcHist([frame], [c], None, [256], [0,256]);\n",
    "            ax2.plot(hist, color=col);\n",
    "\n",
    "        ax2.set_xticks([]);\n",
    "        ax2.set_yticks([]);\n",
    "\n",
    "    fig.suptitle(f\"{cls_name}: Frames and RGB Histograms\", fontsize=14, y=0.995);\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.savefig(f\"{save_dir}/{cls_name}_RGB_frames_hist.png\", dpi=300, bbox_inches=\"tight\");\n",
    "    #plt.show();\n",
    "    plt.close();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9526d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------\n",
    "# 2. HSV color histograms (per frame)\n",
    "#------------------------------------------\n",
    "\n",
    "save_dir = \"../results/feature_visualizations/color_features/HSV\";\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True);\n",
    "\n",
    "for cls_name, idx in selected_indices.items():\n",
    "\n",
    "    frames = extract_frames(train_videos[idx]);\n",
    "    num_frames = len(frames);\n",
    "\n",
    "    fig = plt.figure(figsize=(14, num_frames * 2));\n",
    "\n",
    "    for f in range(num_frames):\n",
    "        frame = frames[f];\n",
    "\n",
    "         # ---- Displaying frame on Left. ----\n",
    "        ax1 = plt.subplot(num_frames, 2, 2*f + 1);\n",
    "        ax1.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB));\n",
    "        ax1.set_title(f\"{cls_name} - F{f+1}\", fontsize=7);\n",
    "        ax1.axis(\"off\");\n",
    "\n",
    "         # ---- Displaying HSV histogram on right. ----\n",
    "        ax2 = plt.subplot(num_frames, 2, 2*f + 2);\n",
    "\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV);\n",
    "        labels = ['Hue','Saturation','Value'];\n",
    "\n",
    "        for i in range(3):\n",
    "            hist = cv2.calcHist([hsv],[i],None,[256],[0,256]);\n",
    "            ax2.plot(hist, label=labels[i]);\n",
    "\n",
    "        ax2.set_xticks([]);\n",
    "        ax2.set_yticks([]);\n",
    "\n",
    "    fig.suptitle(f\"{cls_name}: Frames and HSV Histograms\", fontsize=14, y=0.995);\n",
    "    plt.tight_layout(rect=[0,0,1,0.97]);\n",
    "    plt.subplots_adjust(hspace=0.1, wspace=0.05);\n",
    "    plt.savefig(f\"{save_dir}/{cls_name}_HSV_frames_hist.png\", dpi=300, bbox_inches=\"tight\");\n",
    "    #plt.show();\n",
    "    plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80336f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 3. Average color distribution and Color moments for each video.\n",
    "#    (Selected 1 video from each class.)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "save_dir = \"../results/feature_visualizations/color_features/avg_color_dist_and_color_moments\";\n",
    "os.makedirs(save_dir, exist_ok=True);\n",
    "\n",
    "for cls_name, idx in selected_indices.items():\n",
    "\n",
    "    print(\"\\n*****************************************\");\n",
    "    print(\"Class:\", cls_name);\n",
    "    print(\"Video:\", os.path.basename(train_videos[idx]));\n",
    "    print(\"*****************************************\");\n",
    "    frames = extract_frames(train_videos[idx]);\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # 1. Average color distribution across video\n",
    "    # ----------------------------------------------------------------------------\n",
    "\n",
    "    avg_hist = np.zeros((3,256));\n",
    "\n",
    "    for frame in frames:\n",
    "        for c in range(3):\n",
    "            hist = cv2.calcHist([frame],[c],None,[256],[0,256]).flatten();\n",
    "            avg_hist[c] += hist;\n",
    "\n",
    "    avg_hist = avg_hist / len(frames);\n",
    "\n",
    "    plt.figure(figsize=(8,4));\n",
    "    plt.plot(avg_hist[0], color='b', label='Blue');\n",
    "    plt.plot(avg_hist[1], color='g', label='Green');\n",
    "    plt.plot(avg_hist[2], color='r', label='Red');\n",
    "    plt.title(f\"{cls_name} - Average Color Distribution (Whole Video)\");\n",
    "    plt.xlabel(\"Pixel Intensity\");\n",
    "    plt.ylabel(\"Average Frequency\");\n",
    "    plt.legend();\n",
    "    plt.savefig(f\"{save_dir}/{cls_name}_average_color_distribution.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # 2. Color moments across video\n",
    "    # ----------------------------------------------\n",
    "\n",
    "    all_pixels = [];\n",
    "    for frame in frames:\n",
    "        all_pixels.append(frame.reshape(-1,3));\n",
    "\n",
    "    all_pixels = np.concatenate(all_pixels, axis=0);\n",
    "\n",
    "    mean = np.mean(all_pixels, axis=0);\n",
    "    var  = np.var(all_pixels, axis=0);\n",
    "    skew = np.mean((all_pixels - mean)**3, axis=0);\n",
    "\n",
    "    labels = ['Blue','Green','Red'];\n",
    "    x = np.arange(3);\n",
    "\n",
    "    plt.figure(figsize=(6,4));\n",
    "    plt.bar(x-0.25, mean, 0.25, label='Mean');\n",
    "    plt.bar(x,      var,  0.25, label='Variance');\n",
    "    plt.bar(x+0.25, skew, 0.25, label='Skewness');\n",
    "    plt.xticks(x, labels);\n",
    "    plt.title(f\"{cls_name} - Color Moments (Whole Video)\");\n",
    "    plt.legend();\n",
    "    plt.savefig(f\"{save_dir}/{cls_name}_color_moments.png\", dpi=300, bbox_inches=\"tight\");\n",
    "    #plt.show();\n",
    "    plt.close();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d532822d",
   "metadata": {},
   "source": [
    "2. Texture features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8070fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "#............................................... Low-Level Features............................................#\n",
    "################################################################################################################\n",
    "\n",
    "# ...............................................  Texture features  ........................................... #\n",
    "\n",
    "#------------------------------------------\n",
    "# 1. Gray Level Co-occurrence Matrix (GLCM)\n",
    "#------------------------------------------\n",
    "\n",
    "# ===== save folder =====\n",
    "save_dir = \"../results/feature_visualizations/texture_features/glcm\";\n",
    "os.makedirs(save_dir, exist_ok=True);\n",
    "\n",
    "for cls_name, idx in selected_indices.items():\n",
    "\n",
    "    print(\"\\n==============================\");\n",
    "    print(\"GLCM for class:\", cls_name);\n",
    "\n",
    "    frames = extract_frames(train_videos[idx]);\n",
    "    num_frames = len(frames);\n",
    "\n",
    "    fig = plt.figure(figsize=(14, num_frames * 2));\n",
    "\n",
    "    for f in range(num_frames):\n",
    "        frame = frames[f];\n",
    "        #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY);\n",
    "        gray = cv2.cvtColor((frame * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # ---- Computing GLCM ----\n",
    "        glcm = graycomatrix(gray,\n",
    "                            distances=[1],\n",
    "                            angles=[0],\n",
    "                            levels=256,\n",
    "                            symmetric=True,\n",
    "                            normed=True);\n",
    "\n",
    "        contrast = graycoprops(glcm, 'contrast')[0][0];\n",
    "        homogeneity = graycoprops(glcm, 'homogeneity')[0][0];\n",
    "        energy = graycoprops(glcm, 'energy')[0][0];\n",
    "        correlation = graycoprops(glcm, 'correlation')[0][0];\n",
    "\n",
    "        # ---- Displaying Gray scale image on Left Hand Side ----\n",
    "        ax1 = plt.subplot(num_frames, 2, 2*f + 1);\n",
    "        ax1.imshow(gray, cmap=\"gray\");\n",
    "        ax1.set_title(f\"{cls_name} F{f+1}\", fontsize=7);\n",
    "        ax1.axis(\"off\");\n",
    "\n",
    "        # ---- Displaying GLCM properties on Right Hand Side----\n",
    "        ax2 = plt.subplot(num_frames, 2, 2*f + 2);\n",
    "\n",
    "        labels = [\"Contrast\",\"Homogeneity\",\"Energy\",\"Correlation\"];\n",
    "        #-------------- Normalizing the values to 0-1 ------------------\n",
    "        values = np.array([contrast, homogeneity, energy, correlation]);\n",
    "        values = values / np.max(values);\n",
    "\n",
    "        x = np.arange(len(labels));\n",
    "\n",
    "        ax2.bar(x, values);\n",
    "        ax2.set_xticks(x);\n",
    "        ax2.set_xticklabels(labels, rotation=45, fontsize=6);\n",
    "\n",
    "        ax2.set_yscale(\"log\");\n",
    "\n",
    "    fig.suptitle(f\"{cls_name}: GLCM Texture Properties (30 Frames)\", fontsize=14);\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.25, wspace=0.1, top=0.95);\n",
    "\n",
    "    plt.savefig(f\"{save_dir}/{cls_name}_GLCM_frames.png\",dpi=300, bbox_inches=\"tight\");\n",
    "    #plt.show();\n",
    "    plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f84897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------\n",
    "# 2. Local Binary Patterns (LBP)\n",
    "#------------------------------------------\n",
    "\n",
    "# ===== save folder =====\n",
    "save_dir = \"../results/feature_visualizations/texture_features/lbp\";\n",
    "os.makedirs(save_dir, exist_ok=True);\n",
    "\n",
    "# LBP parameters\n",
    "P = 8   # number of neighbors\n",
    "R = 1   # radius\n",
    "\n",
    "for cls_name, idx in selected_indices.items():\n",
    "\n",
    "    print(\"\\n==============================\");\n",
    "    print(\"LBP for class:\", cls_name);\n",
    "\n",
    "    frames = extract_frames(train_videos[idx]);\n",
    "    num_frames = len(frames);\n",
    "\n",
    "    fig = plt.figure(figsize=(16, num_frames * 5));\n",
    "\n",
    "    for f in range(num_frames):\n",
    "        frame = frames[f];\n",
    "        #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY);\n",
    "        gray = cv2.cvtColor((frame * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        lbp = local_binary_pattern(gray, P, R, method=\"uniform\");\n",
    "\n",
    "        # ---- Displaying Gray scale image on Left Hand Side ----\n",
    "        ax1 = plt.subplot(num_frames, 2, 2*f + 1);\n",
    "        ax1.imshow(gray, cmap=\"gray\");\n",
    "        ax1.set_title(f\"{cls_name} Frame:{f+1}\", fontsize=7);\n",
    "        ax1.axis(\"off\");\n",
    "\n",
    "        # ---- Displaying LBP texture map on Right Hand Side ----\n",
    "        ax2 = plt.subplot(num_frames, 2, 2*f + 2);\n",
    "        ax2.imshow(lbp, cmap=\"gray\");\n",
    "        ax2.set_title(\"LBP\", fontsize=7);\n",
    "        ax2.axis(\"off\");\n",
    "\n",
    "    fig.suptitle(f\"{cls_name}: Local Binary Pattern Texture Maps (30 Frames)\", fontsize=14);\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0.05, top=0.95);\n",
    "\n",
    "    plt.savefig(f\"{save_dir}/{cls_name}_LBP_frames.png\", dpi=300, bbox_inches=\"tight\");\n",
    "    #plt.show();\n",
    "    plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d18ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------\n",
    "# 3. Gabor filter responses\n",
    "#------------------------------------------\n",
    "\n",
    "\n",
    "# ===== save folder =====\n",
    "save_dir = \"../results/feature_visualizations/texture_features/gabor\";\n",
    "os.makedirs(save_dir, exist_ok=True);\n",
    "\n",
    "# Gabor parameters\n",
    "frequency = 0.6;\n",
    "\n",
    "for cls_name, idx in selected_indices.items():\n",
    "\n",
    "    print(\"\\n==============================\");\n",
    "    print(\"Gabor for class:\", cls_name);\n",
    "\n",
    "    frames = extract_frames(train_videos[idx]);\n",
    "    num_frames = len(frames);\n",
    "\n",
    "    fig = plt.figure(figsize=(16, num_frames * 5));\n",
    "\n",
    "    for f in range(num_frames):\n",
    "        frame = frames[f];\n",
    "        #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY);\n",
    "        gray = cv2.cvtColor((frame * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        real, imag = gabor(gray, frequency=frequency);\n",
    "\n",
    "        # ---- Displaying Gray scale image on Left Hand Side ----\n",
    "        ax1 = plt.subplot(num_frames, 2, 2*f + 1);\n",
    "        ax1.imshow(gray, cmap=\"gray\");\n",
    "        ax1.set_title(f\"{cls_name} F{f+1}\", fontsize=7);\n",
    "        ax1.axis(\"off\");\n",
    "\n",
    "         # ---- Displaying Gabor response on Right Hand Side ----\n",
    "        ax2 = plt.subplot(num_frames, 2, 2*f + 2);\n",
    "        ax2.imshow(real, cmap=\"gray\");\n",
    "        ax2.set_title(\"Gabor response\", fontsize=7);\n",
    "        ax2.axis(\"off\");\n",
    "\n",
    "    fig.suptitle(f\"{cls_name}: Gabor Filter Texture Responses (30 Frames)\", fontsize=14);\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0.05, top=0.95);\n",
    "\n",
    "    plt.savefig(f\"{save_dir}/{cls_name}_Gabor_frames.png\",\n",
    "                dpi=300, bbox_inches=\"tight\");\n",
    "    #plt.show();\n",
    "    plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867de6af",
   "metadata": {},
   "source": [
    "3. Shape features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "#............................................... Low-Level Features............................................#\n",
    "################################################################################################################\n",
    "\n",
    "# ...............................................  Shape features  ........................................... #\n",
    "\n",
    "\n",
    "#-----------------------------------------------\n",
    "# 1. Edge histograms using Canny edge detection\n",
    "#-----------------------------------------------\n",
    "\n",
    "# ===== save folder =====\n",
    "save_dir = \"../results/feature_visualizations/shape_features/canny_edges\";\n",
    "os.makedirs(save_dir, exist_ok=True);\n",
    "\n",
    "\n",
    "for cls_name, idx in selected_indices.items():\n",
    "\n",
    "    frames = extract_frames(train_videos[idx]);\n",
    "    num_frames = len(frames);\n",
    "\n",
    "    fig = plt.figure(figsize=(16, num_frames * 5));\n",
    "\n",
    "    for f in range(num_frames):\n",
    "        frame = frames[f];\n",
    "        \n",
    "        #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY);\n",
    "        gray = cv2.cvtColor((frame * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "        edges = cv2.Canny(gray, 100, 200);\n",
    "\n",
    "        # ---- Displaying Gray scale image on Left Hand Side ----\n",
    "        ax1 = plt.subplot(num_frames, 2, 2*f + 1);\n",
    "        ax1.imshow(gray, cmap=\"gray\");\n",
    "        ax1.set_title(f\"{cls_name} Frame: {f+1}\", fontsize=7);\n",
    "        ax1.axis(\"off\");\n",
    "\n",
    "        # ---- Displaying Canny Edges on Right Hand Side ----\n",
    "        ax2 = plt.subplot(num_frames, 2, 2*f + 2);\n",
    "        ax2.imshow(edges, cmap=\"gray\");\n",
    "        ax2.set_title(\"Canny edges\", fontsize=7);\n",
    "        ax2.axis(\"off\");\n",
    "\n",
    "    fig.suptitle(f\"{cls_name}: Shape Features: Canny Edge Detection\", fontsize=14);\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0.05, top=0.95);\n",
    "\n",
    "    plt.savefig(f\"{save_dir}/{cls_name}_Canny_Edges.png\", dpi=300, bbox_inches=\"tight\");\n",
    "    #plt.show();\n",
    "    plt.close(); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d84e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------\n",
    "# 2. Contour-based features\n",
    "#-----------------------------------------------\n",
    "\n",
    "# ===== save folder =====\n",
    "save_dir = \"../results/feature_visualizations/shape_features/contours\";\n",
    "os.makedirs(save_dir, exist_ok=True);\n",
    "\n",
    "for cls_name, idx in selected_indices.items():\n",
    "\n",
    "    print(\"\\n==============================\");\n",
    "    print(\"Contours for class:\", cls_name);\n",
    "\n",
    "    frames = extract_frames(train_videos[idx]);\n",
    "    num_frames = len(frames);\n",
    "\n",
    "    fig = plt.figure(figsize=(16, num_frames * 5));\n",
    "\n",
    "    for f in range(num_frames):\n",
    "        frame = frames[f];\n",
    "        #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY);\n",
    "        gray = cv2.cvtColor((frame * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # ---- preprocessing stage ----\n",
    "        blur = cv2.GaussianBlur(gray, (5,5), 0);\n",
    "        edges = cv2.Canny(blur, 80, 160);\n",
    "\n",
    "        # ---- finding contours ----\n",
    "        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE);\n",
    "\n",
    "        # ---- draw contours ----\n",
    "        contour_img = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR);\n",
    "        cv2.drawContours(contour_img, contours, -1, (0,255,0), 1);\n",
    "\n",
    "        # ---- Displaying Gray scale image on Left Hand Side ----\n",
    "        ax1 = plt.subplot(num_frames, 2, 2*f + 1);\n",
    "        ax1.imshow(gray, cmap=\"gray\");\n",
    "        ax1.set_title(f\"{cls_name} F{f+1}\", fontsize=7);\n",
    "        ax1.axis(\"off\");\n",
    "\n",
    "        # ---- Displaying contours on Right Hand Side ----\n",
    "        ax2 = plt.subplot(num_frames, 2, 2*f + 2);\n",
    "        ax2.imshow(cv2.cvtColor(contour_img, cv2.COLOR_BGR2RGB));\n",
    "        ax2.set_title(\"Contours\", fontsize=7);\n",
    "        ax2.axis(\"off\");\n",
    "\n",
    "    fig.suptitle(f\"{cls_name}: Contour-Based Shape Features (30 Frames)\", fontsize=14);\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0.05, top=0.95);\n",
    "\n",
    "    plt.savefig(f\"{save_dir}/{cls_name}_Contours.png\",dpi=300, bbox_inches=\"tight\");\n",
    "    #plt.show();\n",
    "    plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca5657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------\n",
    "# 3. HOG (Histogram of Oriented Gradients)\n",
    "#-----------------------------------------------\n",
    "\n",
    "# ===== save folder =====\n",
    "save_dir = \"../results/feature_visualizations/shape_features/hog\";\n",
    "os.makedirs(save_dir, exist_ok=True);\n",
    "\n",
    "for cls_name, idx in selected_indices.items():\n",
    "\n",
    "    print(\"\\n==============================\");\n",
    "    print(\"HOG for class:\", cls_name);\n",
    "\n",
    "    frames = extract_frames(train_videos[idx]);\n",
    "    num_frames = len(frames);\n",
    "\n",
    "    fig = plt.figure(figsize=(16, num_frames * 5));\n",
    "\n",
    "    for f in range(num_frames):\n",
    "        frame = frames[f];\n",
    "        #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY);\n",
    "        gray = cv2.cvtColor((frame * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # ---- Computing HOG ----\n",
    "        hog_features, hog_image = hog(gray, orientations=9, pixels_per_cell=(8,8), cells_per_block=(2,2), visualize=True);\n",
    "\n",
    "        # ---- Displaying Gray scale image on Left Hand Side ----\n",
    "        ax1 = plt.subplot(num_frames, 2, 2*f + 1);\n",
    "        ax1.imshow(gray, cmap=\"gray\");\n",
    "        ax1.set_title(f\"{cls_name} F{f+1}\", fontsize=7);\n",
    "        ax1.axis(\"off\");\n",
    "\n",
    "        # ---- Displaying HOG visualization on Right Hand Side ----\n",
    "        ax2 = plt.subplot(num_frames, 2, 2*f + 2);\n",
    "        ax2.imshow(hog_image, cmap=\"gray\");\n",
    "        ax2.set_title(\"HOG map\", fontsize=7);\n",
    "        ax2.axis(\"off\");\n",
    "\n",
    "    fig.suptitle(f\"{cls_name}: Shape Features using HOG (30 Frames)\", fontsize=14);\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0.05, top=0.95);\n",
    "\n",
    "    plt.savefig(f\"{save_dir}/{cls_name}_HOG.png\", dpi=300, bbox_inches=\"tight\");\n",
    "    #plt.show();\n",
    "    plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa15fe9",
   "metadata": {},
   "source": [
    "Motion Features:\n",
    "1. Frame Differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd69319",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "#............................................... Motion Features............................................#\n",
    "################################################################################################################\n",
    "\n",
    "#-----------------------------------------------\n",
    "# Frame Differencing\n",
    "#-----------------------------------------------\n",
    "\n",
    "# ===== save folder =====\n",
    "save_dir = \"../results/feature_visualizations/motion_features/frame_differencing\";\n",
    "os.makedirs(save_dir, exist_ok=True);\n",
    "\n",
    "for cls_name, idx in selected_indices.items():\n",
    "\n",
    "    print(\"\\n==============================\");\n",
    "    print(\"Frame differencing for class:\", cls_name);\n",
    "\n",
    "    frames = extract_frames(train_videos[idx]);\n",
    "    num_frames = len(frames);\n",
    "\n",
    "    fig = plt.figure(figsize=(16, (num_frames-1) * 5));\n",
    "\n",
    "    for f in range(1, num_frames):\n",
    "        #prev = cv2.cvtColor(frames[f-1], cv2.COLOR_BGR2GRAY);\n",
    "        #curr = cv2.cvtColor(frames[f], cv2.COLOR_BGR2GRAY);\n",
    "\n",
    "        prev = cv2.cvtColor((frames[f-1] * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "        curr = cv2.cvtColor((frames[f] * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # ---- calculating absolute difference ----\n",
    "        diff = cv2.absdiff(curr, prev);\n",
    "\n",
    "        # ---- calculating statistics ----\n",
    "        mean_diff = np.mean(diff);\n",
    "        std_diff  = np.std(diff);\n",
    "\n",
    "        # ---- Displaying Gray scale image on Left Hand Side ----\n",
    "        ax1 = plt.subplot(num_frames-1, 3, 3*(f-1) + 1);\n",
    "        #ax1.imshow(curr, cmap=\"gray\");\n",
    "        ax1.imshow(curr.astype(np.uint8), cmap=\"gray\")\n",
    "        ax1.set_title(f\"{cls_name} Frame: {f+1}\", fontsize=7);\n",
    "        ax1.axis(\"off\");\n",
    "\n",
    "        # ---- Displaying frame difference in middle -----\n",
    "        ax2 = plt.subplot(num_frames-1, 3, 3*(f-1) + 2);\n",
    "        #ax2.imshow(diff, cmap=\"gray\");\n",
    "        ax2.imshow(diff.astype(np.uint8), cmap=\"gray\")\n",
    "        ax2.set_title(\"Frame difference\", fontsize=7);\n",
    "        ax2.axis(\"off\");\n",
    "\n",
    "        # ---- Displaying motion intensity histogram on Right Hand Side ----\n",
    "        ax3 = plt.subplot(num_frames-1, 3, 3*(f-1) + 3);\n",
    "        hist = cv2.calcHist([diff],[0],None,[256],[0,256]);\n",
    "        ax3.plot(hist, color=\"black\");\n",
    "        ax3.set_title(f\"Mean={mean_diff:.1f}, Std={std_diff:.1f}\", fontsize=7);\n",
    "        ax3.set_xticks([]); ax3.set_yticks([]);\n",
    "\n",
    "    fig.suptitle(f\"{cls_name}: Motion via Frame Differencing\", fontsize=14);\n",
    "    plt.subplots_adjust(hspace=0.25, wspace=0.15, top=0.95);\n",
    "\n",
    "    plt.savefig(f\"{save_dir}/{cls_name}_Frame_Difference.png\", dpi=300, bbox_inches=\"tight\");\n",
    "    #plt.show();\n",
    "    plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8105d36",
   "metadata": {},
   "source": [
    "2. Optical Flow\n",
    "- Lucas-Kanade sparse optical flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4964501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------\n",
    "# 1. Lucas-Kanade sparse optical flow\n",
    "#-----------------------------------------------\n",
    "\n",
    "# ===== save folder =====\n",
    "save_dir = \"../results/feature_visualizations/motion_features/optical_flow/lukas-kanade\";\n",
    "os.makedirs(save_dir, exist_ok=True);\n",
    "\n",
    "# Shi-Tomasi corner detection parameters (To detect the motion points.)\n",
    "feature_params = dict(maxCorners=200,\n",
    "                      qualityLevel=0.3,\n",
    "                      minDistance=7,\n",
    "                      blockSize=7);\n",
    "\n",
    "# Lucas-Kanade optical flow parameters (To check the direction of the motion)\n",
    "lk_params = dict(winSize=(15,15),\n",
    "                 maxLevel=2,\n",
    "                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03));\n",
    "\n",
    "\n",
    "for cls_name, idx in selected_indices.items():\n",
    "\n",
    "    print(\"\\n==============================\");\n",
    "    print(\"Lucas-Kanade optical flow for:\", cls_name);\n",
    "\n",
    "    frames = extract_frames(train_videos[idx]);\n",
    "    num_frames = len(frames);\n",
    "\n",
    "    # Taking first frame\n",
    "    #old_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY);\n",
    "    old_gray = cv2.cvtColor((frames[0] * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "    p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params);\n",
    "\n",
    "    fig = plt.figure(figsize=(20, (num_frames-1) * 8));\n",
    "\n",
    "    for f in range(1, num_frames):\n",
    "        frame = frames[f];\n",
    "        #frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY);\n",
    "        frame_gray = cv2.cvtColor((frame * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Checking optical flow\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params);\n",
    "\n",
    "        #vis = frame.copy();\n",
    "\n",
    "        vis = (frame * 255).astype(np.uint8).copy()\n",
    "\n",
    "\n",
    "        if p1 is not None:\n",
    "            good_new = p1[st==1];\n",
    "            good_old = p0[st==1];\n",
    "\n",
    "            for (new, old) in zip(good_new, good_old):\n",
    "                a,b = new.ravel();\n",
    "                c,d = old.ravel();\n",
    "                scale = 3   # setting exaggeration factor\n",
    "                ax = int(c + scale*(a-c))\n",
    "                ay = int(d + scale*(b-d))\n",
    "\n",
    "                cv2.arrowedLine(vis, (int(c),int(d)), (ax,ay), (0,255,255), 2, tipLength=0.2)\n",
    "\n",
    "                cv2.circle(vis, (int(a),int(b)), 2, (0,0,255), -1);\n",
    "\n",
    "            p0 = good_new.reshape(-1,1,2);\n",
    "\n",
    "        # ---- drawing the plot ----\n",
    "        ax = plt.subplot(num_frames-1, 1, f);\n",
    "        ax.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB));\n",
    "        ax.set_title(f\"{cls_name} - Optical Flow (Frame {f} to Frame {f+1})\", fontsize=8);\n",
    "        ax.axis(\"off\");\n",
    "\n",
    "        old_gray = frame_gray.copy();\n",
    "\n",
    "    fig.suptitle(f\"{cls_name}: Lucas-Kanade Sparse Optical Flow\", fontsize=14);\n",
    "    plt.subplots_adjust(hspace=0.2, top=0.95);\n",
    "\n",
    "    plt.savefig(f\"{save_dir}/{cls_name}_LucasKanade.png\", dpi=300, bbox_inches=\"tight\");\n",
    "    #plt.show();\n",
    "    plt.close();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21523c68",
   "metadata": {},
   "source": [
    "- Horn-Schunck dense optical flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c179cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------\n",
    "# 2. Horn-Schunck dense optical flow\n",
    "#-----------------------------------------------\n",
    "\n",
    "import cv2, os, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "save_dir = \"../results/feature_visualizations/motion_features/optical_flow/Horn-Schunck\";\n",
    "os.makedirs(save_dir, exist_ok=True);\n",
    "\n",
    "for cls_name, idx in selected_indices.items():\n",
    "\n",
    "    print(\"\\nDense optical flow for:\", cls_name);\n",
    "\n",
    "    frames = extract_frames(train_videos[idx]);\n",
    "    num_frames = len(frames);\n",
    "\n",
    "    #prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY);\n",
    "    prev_gray = cv2.cvtColor((frames[0] * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    fig = plt.figure(figsize=(18, (num_frames-1) * 4));\n",
    "\n",
    "    for f in range(1, num_frames):\n",
    "        #curr_gray = cv2.cvtColor(frames[f], cv2.COLOR_BGR2GRAY);\n",
    "        curr_gray = cv2.cvtColor((frames[f] * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "        # ---- dense optical flow (Horn–Schunck) ----\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0);\n",
    "\n",
    "        mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1]);\n",
    "\n",
    "        # ---- magnitude map ----\n",
    "        mag_norm = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX);\n",
    "        mag_norm = mag_norm.astype(np.uint8);\n",
    "\n",
    "        # ---- direction map (HSV) ----\n",
    "        hsv = np.zeros_like((frames[f] * 255).astype(np.uint8))\n",
    "\n",
    "        hsv[...,1] = 255;\n",
    "        hsv[...,0] = ang * 180 / np.pi / 2;\n",
    "        hsv[...,2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX);\n",
    "        flow_rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB);\n",
    "        flow_rgb = flow_rgb.astype(np.uint8)\n",
    "\n",
    "\n",
    "        # ---- plots ----\n",
    "        ax1 = plt.subplot(num_frames-1, 3, 3*(f-1)+1);\n",
    "        ax1.imshow(curr_gray, cmap=\"gray\");\n",
    "        ax1.set_title(\"Gray frame\", fontsize=7);\n",
    "        ax1.axis(\"off\");\n",
    "\n",
    "        ax2 = plt.subplot(num_frames-1, 3, 3*(f-1)+2);\n",
    "        ax2.imshow(mag_norm, cmap=\"inferno\");\n",
    "        ax2.set_title(\"Motion magnitude\", fontsize=7);\n",
    "        ax2.axis(\"off\");\n",
    "\n",
    "        ax3 = plt.subplot(num_frames-1, 3, 3*(f-1)+3);\n",
    "        ax3.imshow(flow_rgb);\n",
    "        ax3.set_title(\"Motion direction (dense flow)\", fontsize=7);\n",
    "        ax3.axis(\"off\");\n",
    "\n",
    "        prev_gray = curr_gray.copy();\n",
    "\n",
    "    fig.suptitle(f\"{cls_name}: Dense Optical Flow (Horn–Schunck style)\", fontsize=14);\n",
    "    plt.subplots_adjust(hspace=0.25, wspace=0.08, top=0.95);\n",
    "\n",
    "    plt.savefig(f\"{save_dir}/{cls_name}_Dense_OpticalFlow.png\", dpi=300, bbox_inches=\"tight\");\n",
    "    #plt.show();\n",
    "    plt.close();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb8797",
   "metadata": {},
   "source": [
    "- Motion magnitude and direction histograms\n",
    "- Average motion intensity and dominant direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c772173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------\n",
    "# Motion magnitude and direction histograms\n",
    "# Average motion intensity and dominant direction`\n",
    "#-----------------------------------------------\n",
    "\n",
    "save_dir = \"../results/feature_visualizations/motion_features/optical_flow/statistics\";\n",
    "os.makedirs(save_dir, exist_ok=True);\n",
    "\n",
    "for cls_name, idx in selected_indices.items():\n",
    "\n",
    "    print(\"\\nMotion stats for:\", cls_name);\n",
    "\n",
    "    frames = extract_frames(train_videos[idx]);\n",
    "    num_frames = len(frames);\n",
    "\n",
    "    all_mags = [];\n",
    "    all_angs = [];\n",
    "\n",
    "    prev_gray = cv2.cvtColor((frames[0] * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "    for f in range(1, num_frames):\n",
    "        curr_gray = cv2.cvtColor((frames[f] * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0);\n",
    "\n",
    "        mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1]);\n",
    "\n",
    "        all_mags.append(mag.flatten());\n",
    "        all_angs.append(ang.flatten());\n",
    "\n",
    "        prev_gray = curr_gray.copy();\n",
    "\n",
    "    all_mags = np.concatenate(all_mags);\n",
    "    all_angs = np.concatenate(all_angs);\n",
    "\n",
    "    # ---- statistics ----\n",
    "    avg_motion = np.mean(all_mags);\n",
    "    dominant_direction = np.mean(all_angs);\n",
    "\n",
    "    print(\"Average motion intensity:\", avg_motion);\n",
    "    print(\"Dominant direction (radians):\", dominant_direction);\n",
    "\n",
    "    # ---- plots ----\n",
    "    plt.figure(figsize=(12,4));\n",
    "\n",
    "    plt.subplot(1,2,1);\n",
    "    plt.hist(all_mags, bins=50, color=\"red\");\n",
    "    plt.title(f\"{cls_name} - Motion Magnitude Histogram\");\n",
    "    plt.xlabel(\"Magnitude\");\n",
    "    plt.ylabel(\"Frequency\");\n",
    "\n",
    "    plt.subplot(1,2,2);\n",
    "    plt.hist(all_angs, bins=50, color=\"purple\");\n",
    "    plt.title(f\"{cls_name} - Motion Direction Histogram\");\n",
    "    plt.xlabel(\"Direction (radians)\");\n",
    "    plt.ylabel(\"Frequency\");\n",
    "\n",
    "    plt.suptitle(f\"{cls_name}: Optical Flow Statistics\\n\"\n",
    "                 f\"Average Intensity = {avg_motion:.4f}, \"\n",
    "                 f\"Dominant Direction = {dominant_direction:.2f} rad\");\n",
    "\n",
    "    plt.tight_layout();\n",
    "    plt.savefig(f\"{save_dir}/{cls_name}_MotionStats.png\", dpi=300);\n",
    "    #plt.show();\n",
    "    plt.close();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed280ef",
   "metadata": {},
   "source": [
    "3. Motion History Images (MHI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71602ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------\n",
    "# Motion History Images (MHI)\n",
    "# Temporal template creation\n",
    "# Motion energy analysis\n",
    "#-----------------------------------------------\n",
    "\n",
    "save_dir = \"../results/feature_visualizations/motion_features/mhi\";\n",
    "os.makedirs(save_dir, exist_ok=True);\n",
    "\n",
    "for cls_name, idx in selected_indices.items():\n",
    "\n",
    "    print(\"\\nMHI for:\", cls_name);\n",
    "\n",
    "    frames = extract_frames(train_videos[idx]);\n",
    "    num_frames = len(frames);\n",
    "\n",
    "    h, w, _ = frames[0].shape;\n",
    "    mhi = np.zeros((h, w), dtype=np.float32);\n",
    "\n",
    "    #prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY);\n",
    "    prev_gray = cv2.cvtColor((frames[0] * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "    timestamp = 1;\n",
    "    decay = 30;   #motion stay duration\n",
    "\n",
    "    for f in range(1, num_frames):\n",
    "        #curr_gray = cv2.cvtColor(frames[f], cv2.COLOR_BGR2GRAY);\n",
    "        curr_gray = cv2.cvtColor((frames[f] * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        diff = cv2.absdiff(curr_gray, prev_gray);\n",
    "        _, motion_mask = cv2.threshold(diff, 25, 1, cv2.THRESH_BINARY);\n",
    "\n",
    "        # update MHI\n",
    "        mhi[motion_mask == 1] = timestamp;\n",
    "        mhi[motion_mask == 0] -= 1;\n",
    "        mhi[mhi < 0] = 0;\n",
    "\n",
    "        prev_gray = curr_gray.copy();\n",
    "        timestamp += 1;\n",
    "\n",
    "    # normalize for display\n",
    "    mhi_norm = cv2.normalize(mhi, None, 0, 255, cv2.NORM_MINMAX);\n",
    "    mhi_norm = mhi_norm.astype(np.uint8);\n",
    "\n",
    "    # ---- motion energy ----\n",
    "    motion_energy = np.sum(mhi_norm > 0);\n",
    "\n",
    "    print(\"Motion energy:\", motion_energy);\n",
    "\n",
    "    # ---- plots ----\n",
    "    plt.figure(figsize=(10,4));\n",
    "\n",
    "    plt.subplot(1,2,1);\n",
    "    #plt.imshow(cv2.cvtColor(frames[-1], cv2.COLOR_BGR2RGB));\n",
    "    last_vis = (frames[-1] * 255).astype(np.uint8)\n",
    "    plt.imshow(cv2.cvtColor(last_vis, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    plt.title(f\"{cls_name} - Last frame\");\n",
    "    plt.axis(\"off\");\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(mhi_norm, cmap=\"inferno\");\n",
    "    plt.title(f\"MHI (Motion Energy = {motion_energy})\");\n",
    "    plt.axis(\"off\");\n",
    "\n",
    "    plt.suptitle(f\"{cls_name}: Motion History Image\");\n",
    "    plt.tight_layout();\n",
    "\n",
    "    plt.savefig(f\"{save_dir}/{cls_name}_MHI.png\", dpi=300);\n",
    "    #plt.show();\n",
    "    plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc2f02",
   "metadata": {},
   "source": [
    "Temporal Features\n",
    "- Statistical measures of feature sequences (mean, std, min, max)\n",
    "- Frame-to-frame variation analysis\n",
    "- Temporal gradients and patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9881a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "#............................................... Temporal Features............................................#\n",
    "################################################################################################################\n",
    "\n",
    "#-----------------------------------------------\n",
    "# Statistical measures of feature sequences (mean, std, min, max)\n",
    "# Frame-to-frame variation analysis\n",
    "# Temporal gradients and patterns\n",
    "#-----------------------------------------------\n",
    "\n",
    "\n",
    "save_dir = \"../results/feature_visualizations/temporal_features\";\n",
    "os.makedirs(save_dir, exist_ok=True);\n",
    "\n",
    "for cls_name, idx in selected_indices.items():\n",
    "\n",
    "    print(\"\\nTemporal features for:\", cls_name);\n",
    "\n",
    "    frames = extract_frames(train_videos[idx]);\n",
    "    num_frames = len(frames);\n",
    "\n",
    "    #prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY);\n",
    "    prev_gray = cv2.cvtColor((frames[0] * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY);\n",
    "\n",
    "    motion_series = [];\n",
    "\n",
    "    # ---- creating temporal motion sequence ----\n",
    "    for f in range(1, num_frames):\n",
    "        curr_gray = cv2.cvtColor(frames[f], cv2.COLOR_BGR2GRAY);\n",
    "        curr_gray = cv2.cvtColor((frames[f] * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY);\n",
    "\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0);\n",
    "\n",
    "        mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1]);\n",
    "        motion_series.append(np.mean(mag));\n",
    "\n",
    "        prev_gray = curr_gray.copy();\n",
    "\n",
    "    motion_series = np.array(motion_series);\n",
    "\n",
    "    # ---- statistical measures ----\n",
    "    mean_val = np.mean(motion_series);\n",
    "    std_val  = np.std(motion_series);\n",
    "    min_val  = np.min(motion_series);\n",
    "    max_val  = np.max(motion_series);\n",
    "\n",
    "    # ---- checking frame-to-frame variation ----\n",
    "    variation = np.abs(np.diff(motion_series));\n",
    "\n",
    "    # ---- calculating temporal gradient ----\n",
    "    gradient = np.gradient(motion_series);\n",
    "\n",
    "    print(\"Mean:\", mean_val, \"Std:\", std_val, \"Min:\", min_val, \"Max:\", max_val);\n",
    "\n",
    "    # ---- creating plots ----\n",
    "    plt.figure(figsize=(12,6));\n",
    "\n",
    "    plt.subplot(3,1,1);\n",
    "    plt.plot(motion_series, marker=\"o\");\n",
    "    plt.title(f\"{cls_name} - Motion intensity over time\");\n",
    "    plt.ylabel(\"Mean magnitude\");\n",
    "\n",
    "    plt.subplot(3,1,2);\n",
    "    plt.plot(variation, color=\"orange\", marker=\"o\");\n",
    "    plt.title(\"Frame-to-frame variation\");\n",
    "    plt.ylabel(\"Δ motion\");\n",
    "\n",
    "    plt.subplot(3,1,3);\n",
    "    plt.plot(gradient, color=\"green\", marker=\"o\");\n",
    "    plt.title(\"Temporal gradient\");\n",
    "    plt.xlabel(\"Frame index\");\n",
    "    plt.ylabel(\"Gradient\");\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"{cls_name} Temporal Features\\n\"\n",
    "        f\"Mean={mean_val:.4f}, Std={std_val:.4f}, Min={min_val:.4f}, Max={max_val:.4f}\"\n",
    "    );\n",
    "\n",
    "    plt.tight_layout();\n",
    "    plt.savefig(f\"{save_dir}/{cls_name}_TemporalFeatures.png\", dpi=300);\n",
    "    #plt.show();\n",
    "    plt.close();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412339e4",
   "metadata": {},
   "source": [
    "### Extracting all features and building feature matrix for Model Training\n",
    "- This section will take around 15 minutes to complete.\n",
    "- Here, features are extracted from 362 videos.\n",
    "- Train dataset: 271 videos\n",
    "- Test dataset: 46 videos\n",
    "- Validation dataset: 45 videos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b628d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting all the features from all the videos in the dataset.\n",
    "# The extract_all_features Function is using the other functions defined in feature_extraction.py file\n",
    "# It is extracting 4 types of features:\n",
    "# 1. Color Features\n",
    "# 2. Texture Features\n",
    "# 3. Shape Features\n",
    "# 4. Motion Features\n",
    "\n",
    "def extract_all_features(frames):\n",
    "    feat_color   = video_color_features(frames);\n",
    "    feat_texture = video_texture_features(frames);\n",
    "    feat_shape   = video_shape_features(frames);\n",
    "    feat_motion  = temporal_motion_features(frames);\n",
    "\n",
    "    return np.hstack([feat_color, feat_texture, feat_shape, feat_motion]);\n",
    "\n",
    "\n",
    "# Function for creating the feature matrix.\n",
    "#tqdm shows the progress bar for better visualization.\n",
    "\n",
    "def build_feature_matrix(video_list, labels):\n",
    "    X, y = [], [] \n",
    "\n",
    "    for i in tqdm(range(len(video_list))):\n",
    "        frames = extract_frames(video_list[i]);\n",
    "        features = extract_all_features(frames);\n",
    "        X.append(features);\n",
    "        y.append(labels[i]);\n",
    "\n",
    "    return np.array(X), np.array(y);\n",
    "\n",
    "#Variable Representations:\n",
    "# 1. X_train = [video1_features, video2_features, ...]: Represents Feature matrix\n",
    "# 2. shape: (num_videos, num_features) \n",
    "# 3. y_train = class labels (0,1,2) Represents output vector\n",
    "\n",
    "print(\"Building feature matrix for train dataset:\");\n",
    "X_train, y_train = build_feature_matrix(train_videos, train_labels);\n",
    "\n",
    "print(\"\\nBuilding feature matrix for test dataset:\");\n",
    "X_test, y_test   = build_feature_matrix(test_videos, test_labels);\n",
    "\n",
    "print(\"\\nBuilding feature matrix for validation dataset:\");\n",
    "X_val, y_val     = build_feature_matrix(val_videos, val_labels);\n",
    "\n",
    "\n",
    "print(\"\\nPrinting dataset shape: (Number of videos, number of features extracted)\\n\");\n",
    "print(\"Train:\", X_train.shape);\n",
    "print(\"Test: \", X_test.shape);\n",
    "print(\"Validation:  \", X_val.shape);\n",
    "\n",
    "\n",
    "# ===================================================\n",
    "# Feature Quality Enhancement.\n",
    "# Handling missing / invalid feature values\n",
    "# NaN (Not a Number)\n",
    "# +Inf (Nositive infinity)\n",
    "# -Inf (Negative infinity)\n",
    "# ===================================================\n",
    "\n",
    "X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0);\n",
    "X_test  = np.nan_to_num(X_test,  nan=0.0, posinf=0.0, neginf=0.0);\n",
    "X_val   = np.nan_to_num(X_val,   nan=0.0, posinf=0.0, neginf=0.0);\n",
    "\n",
    "print(\"\\nNaN and Inf values (if any) have been handled in all feature matrices.\");\n",
    "\n",
    "\n",
    "# Saving the built feature matrices:\n",
    "\n",
    "np.save(\"../results/saved_feature_matrices/X_train.npy\", X_train);\n",
    "np.save(\"../results/saved_feature_matrices/X_val.npy\", X_val);\n",
    "np.save(\"../results/saved_feature_matrices/X_test.npy\", X_test);\n",
    "\n",
    "np.save(\"../results/saved_feature_matrices/y_train.npy\", y_train);\n",
    "np.save(\"../results/saved_feature_matrices/y_val.npy\", y_val);\n",
    "np.save(\"../results/saved_feature_matrices/y_test.npy\", y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ba8c3",
   "metadata": {},
   "source": [
    "### Classical Machine Learning Algorithms\n",
    "\n",
    "Implementing following Classical Machine Learning Models\n",
    "- Support Vector Machines (SVM)\n",
    "- Random Forest\n",
    "- k-Nearest Neighbors (k-NN)\n",
    "- Logistic Regression\n",
    "- Gradient Boosting (XGBoost or LightGBM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda5fd2",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "1. Support Vector Machines (SVM)\n",
    "- Linear and RBF kernels\n",
    "- Hyperparameter tuning using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# Feature normalization for distance-based models\n",
    "# (Required for SVM, Logistic Regression, KNN)\n",
    "# ===================================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "#Creating pipeline:\n",
    "# Support Vector Machine (Linear Kernel):\n",
    "\n",
    "linear_svm = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm\", SVC(kernel=\"linear\", probability=True, random_state=42))\n",
    "]);\n",
    "\n",
    "#Creating pipeline:\n",
    "# Support Vector Machine (RBF Kernel):\n",
    "\n",
    "rbf_svm = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm\", SVC(kernel=\"rbf\", gamma=\"scale\", probability=True, random_state=42))\n",
    "]);\n",
    "\n",
    "#Training the models:\n",
    "# Linear SVM:\n",
    "linear_svm.fit(X_train, y_train);\n",
    "\n",
    "# Linear SVM:\n",
    "rbf_svm.fit(X_train, y_train);\n",
    "\n",
    "# Collecting the predicted outputs:\n",
    "pred_lin = linear_svm.predict(X_val);\n",
    "pred_rbf = rbf_svm.predict(X_val);\n",
    "\n",
    "# Validation check: (Linear SVM)\n",
    "print(\"Linear SVM (VAL):\", accuracy_score(y_val, pred_lin));\n",
    "print(classification_report(y_val, pred_lin));\n",
    "\n",
    "# Validation check: (RBF SVM)\n",
    "print(\"RBF SVM (VAL):\", accuracy_score(y_val, pred_rbf));\n",
    "print(classification_report(y_val, pred_rbf));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c672686",
   "metadata": {},
   "source": [
    "SVM: Hyperparameter Tuning\n",
    "- This section will take around 5 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2cb1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal is to automatically find the best SVM configuration instead of choosing parameters manually.\n",
    "# SVM performance depends heavily on hyperparameters, mainly:\n",
    "# C → controls margin softness (regularization)\n",
    "# gamma → controls how flexible the RBF decision boundary is\n",
    "# Wrong values can cause:\n",
    "# Underfitting (model too simple)\n",
    "# Overfitting (model too complex)\n",
    "# So we systematically search for the best combination.\n",
    "\n",
    "\n",
    "#This defines the search space of hyperparameters.\n",
    "param_grid = {\n",
    "    \"svm__C\": [0.1, 1, 10],\n",
    "    \"svm__gamma\": [\"scale\", 0.01]\n",
    "};\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm\", SVC(kernel=\"rbf\", probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "# Try all combinations of C and gamma\n",
    "# For each combination:\n",
    "# Split training data into 5 folds\n",
    "# Train on 4 folds, validate on 1 fold\n",
    "# Repeat 5 times\n",
    "# Take average accuracy\n",
    "# Use parallel CPU cores to speed up\n",
    "# So instead of trusting one split, the model is evaluated on multiple internal validations, making the selection reliable.\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=4, \n",
    "    verbose=2\n",
    ");\n",
    "\n",
    "# At this point, the system:\n",
    "# Trains many SVMs\n",
    "# Evaluates them using 5-fold cross-validation\n",
    "# Selects the configuration that gives the highest mean validation accuracy\n",
    "\n",
    "grid.fit(X_train, y_train);\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_);\n",
    "print(\"Best CV Accuracy:\", grid.best_score_);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28298e0e",
   "metadata": {},
   "source": [
    "Getting the best SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f40190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section gives the best SVM:\n",
    "# Now best_svm is the tuned SVM model.ValueError\n",
    "# Already trained on the full training set with the optimal hyperparameters.\n",
    "best_svm = grid.best_estimator_;\n",
    "\n",
    "# The tuned SVM is applied to the separate validation set\n",
    "# These samples were not used for fitting the model parameters\n",
    "\n",
    "val_pred = best_svm.predict(X_val);\n",
    "\n",
    "#This evaluates:\n",
    "# Overall accuracy\n",
    "# Per-class precision, recall, and F1-score\n",
    "# Here we can check:\n",
    "#      - Which actions are well recognized.\n",
    "#      - Which actions are being confused.\n",
    "#      - Whether tuning actually improved generalization.\n",
    "\n",
    "print(\"Tuned SVM (VAL):\", accuracy_score(y_val, val_pred));\n",
    "print(classification_report(y_val, val_pred));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e1724",
   "metadata": {},
   "source": [
    "SVM - Final report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL STAGE: Train on Full Development Set & Test on Unseen Data\n",
    "# This block is responsible for producing the final reported results.\n",
    "\n",
    "# 1. Merging Training and Validation Sets\n",
    "\n",
    "    # vstack → vertically stacks feature matrices\n",
    "    # hstack → joins label vectors\n",
    "    # Therefor now:\n",
    "        # X_final contains all samples from both training and validation sets\n",
    "        # y_final contains their corresponding labels\n",
    "\n",
    "X_final = np.vstack([X_train, X_val]);\n",
    "y_final = np.hstack([y_train, y_val]);\n",
    "\n",
    "# 2. Final Model Training\n",
    "    # This trains the tuned SVM model from scratch using:\n",
    "        # Optimal hyperparameters (from GridSearch)\n",
    "        # The full development dataset (train + validation)\n",
    "    # This gives final production model.\n",
    "\n",
    "start = time.perf_counter()\n",
    "best_svm.fit(X_final, y_final)\n",
    "svm_train_time = time.perf_counter() - start\n",
    "\n",
    "# 3. Prediction on Test Set (True Unseen Data)\n",
    "# The test set is:\n",
    "    # Never used in training\n",
    "    # Never used in hyperparameter tuning\n",
    "    # Never used in validation\n",
    "# So this step measures true generalization performance.\n",
    "\n",
    "start = time.perf_counter()\n",
    "svm_test_pred = best_svm.predict(X_test)\n",
    "svm_test_time = time.perf_counter() - start\n",
    "\n",
    "# 4. Final Performance Metric\n",
    "# This outputs:\n",
    "    # Overall accuracy\n",
    "    # Precision per class\n",
    "    # Recall per class\n",
    "    # F1-score per class\n",
    "\n",
    "print(\"Final Test Accuracy:\", accuracy_score(y_test, svm_test_pred));\n",
    "print(classification_report(y_test, svm_test_pred));\n",
    "\n",
    "# 5. Confusion Matrix Visualization\n",
    "# This visualizes:\n",
    "    # Correct predictions (diagonal)\n",
    "    # Misclassifications (off-diagonal)\n",
    "# It helps analyze:\n",
    "    # Which actions are confused\n",
    "    # Which actions are most reliably recognized\n",
    "# This is critical for:\n",
    "    # discussion section\n",
    "    # comparative analysis\n",
    "    # error analysis\n",
    "\n",
    "save_path = \"../results/confusion_matrices/classical/svm_confusion_matrix.png\";\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test, svm_test_pred);\n",
    "plt.title(\"SVM: Final Test Confusion Matrix\");\n",
    "plt.tight_layout();\n",
    "plt.savefig(save_path, dpi=300);\n",
    "plt.show();\n",
    "plt.close();\n",
    "\n",
    "\n",
    "\n",
    "np.save(\"../results/stats_classical/svm_train_time.npy\", svm_train_time);\n",
    "np.save(\"../results/stats_classical/svm_test_time.npy\", svm_test_time);\n",
    "\n",
    "svm_acc = accuracy_score(y_test, svm_test_pred);\n",
    "svm_f1  = f1_score(y_test, svm_test_pred, average=\"macro\");\n",
    "\n",
    "np.save(\"../results/stats_classical/svm_accuracy.npy\", svm_acc);\n",
    "np.save(\"../results/stats_classical/svm_f1.npy\", svm_f1);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c60828c",
   "metadata": {},
   "source": [
    "SVM: Error Analysis – Identify failure cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd5f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices where the model prediction is wrong i.e., predicted label is not equal to true label.\n",
    "wrong_idx = np.where(y_test != svm_test_pred)[0];\n",
    "\n",
    "# Print total number of misclassified test samples\n",
    "print(\"\\nNumber of misclassified test videos:\", len(wrong_idx));\n",
    "\n",
    "# Display details of a few wrong predictions (first 5 only)\n",
    "# This helps in manually inspecting failure cases\n",
    "for i in wrong_idx:\n",
    "\n",
    "    # Print the video file path\n",
    "    print(\"Video:\", test_videos[i]);\n",
    "\n",
    "    # Print the true class label and the model's predicted label\n",
    "    print(\"True Label:\", y_test[i], \"Predicted Label:\", svm_test_pred[i]);\n",
    "\n",
    "    # Print a separator line for better readability\n",
    "    print(\"-\" * 50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d23a7",
   "metadata": {},
   "source": [
    "2. Random Forest\n",
    "   - Feature importance analysis\n",
    "   - Tree depth and ensemble size optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193633e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implementing PCA (Principle Component Analysis):\n",
    "pca = PCA(n_components=0.95, random_state=42);\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train);\n",
    "X_val_pca   = pca.transform(X_val);\n",
    "X_test_pca_rf  = pca.transform(X_test);\n",
    "\n",
    "print(\"Original feature dimension:\", X_train.shape[1]);\n",
    "print(\"Reduced feature dimension:\", X_train_pca.shape[1]);\n",
    "\n",
    "print(np.cumsum(pca.explained_variance_ratio_));\n",
    "\n",
    "\n",
    "# Baseline Random Forest\n",
    "    # This creates a basic Random Forest model.\n",
    "# n_estimators=100\n",
    "    # → builds 100 decision trees\n",
    "# max_depth=8 → trees are allowed to grow only till level 8\n",
    "# random_state=42 → makes results reproducible\n",
    "# n_jobs=28 → uses 28 CPU cores in parallel\n",
    "\n",
    "rf_base = RandomForestClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=8,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=3,\n",
    "    random_state=42,\n",
    "    n_jobs=28\n",
    ")\n",
    "\n",
    "# Train all trees using the training data.\n",
    "# Each tree:\n",
    "    # Sees a random subset of samples\n",
    "    # Sees a random subset of features\n",
    "    # This randomness reduces overfitting.\n",
    "\n",
    "rf_base.fit(X_train_pca, y_train);\n",
    "\n",
    "# Predicts class labels for validation videos.\n",
    "val_pred_base = rf_base.predict(X_val_pca);\n",
    "\n",
    "\n",
    "# Evaluate how well the baseline forest generalizes.\n",
    "# This gives a reference before tuning.\n",
    "print(\"\\nBaseline RF Validation Accuracy:\", accuracy_score(y_val, val_pred_base));\n",
    "print(classification_report(y_val, val_pred_base));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b61db0",
   "metadata": {},
   "source": [
    "Random Forest:\n",
    "\n",
    "Hyperparameter tuning. \n",
    " - (Tree depth & number of trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c125a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid defines the hyperparameter search space.\n",
    "# n_estimators → number of trees\n",
    "# max_depth → maximum depth of each tree\n",
    "# min_samples_split → minimum samples needed to split a node\n",
    "# min_samples_leaf → minimum samples required at a leaf\n",
    "# These control:\n",
    "    # model complexity\n",
    "    # overfitting vs generalization\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [5, 8, 12],\n",
    "    \"min_samples_split\": [5, 10],\n",
    "    \"min_samples_leaf\": [2, 4]\n",
    "}\n",
    "\n",
    "\n",
    "# Creates a fresh Random Forest model to be optimized.\n",
    "rf = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=28\n",
    ");\n",
    "\n",
    "\n",
    "# This sets up grid search:\n",
    "    # tries all parameter combinations\n",
    "    # performs 5-fold cross-validation\n",
    "    # evaluates average accuracy\n",
    "    # runs jobs in parallel\n",
    "grid_rf = GridSearchCV(\n",
    "    rf,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=4,\n",
    "    verbose=2\n",
    ");\n",
    "\n",
    "# Now the system automatically:\n",
    "# trains many forests\n",
    "# evaluates them internally\n",
    "# selects the best performing configuration\n",
    "\n",
    "grid_rf.fit(X_train, y_train);\n",
    "\n",
    "\n",
    "# Show:\n",
    "    # best tree depth.\n",
    "    # best number of trees.\n",
    "    # best achieved cross-validation accuracy.\n",
    "    \n",
    "print(\"Best RF Parameters:\", grid_rf.best_params_);\n",
    "print(\"Best CV Accuracy:\", grid_rf.best_score_);\n",
    "\n",
    "\n",
    "grid_rf.fit(X_train_pca, y_train);\n",
    "\n",
    "# Tuned model validation\n",
    "# Extracts the best Random Forest model found.\n",
    "best_rf = grid_rf.best_estimator_;\n",
    "\n",
    "# Applies tuned model on validation set.\n",
    "val_pred_rf = best_rf.predict(X_val_pca);\n",
    "\n",
    "# Checks whether tuning improved generalization.\n",
    "print(\"Tuned RF Validation Accuracy:\", accuracy_score(y_val, val_pred_rf));\n",
    "print(classification_report(y_val, val_pred_rf));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c38e6",
   "metadata": {},
   "source": [
    "Random Forest: Final training & test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb425ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge training and validation sets to maximize learning data.\n",
    "\n",
    "X_final = np.vstack([X_train_pca, X_val_pca]);\n",
    "y_final = np.hstack([y_train, y_val]);\n",
    "\n",
    "# Retrains the tuned model on the full development set.\n",
    "start = time.perf_counter()\n",
    "best_rf.fit(X_final, y_final);\n",
    "rf_train_time = time.perf_counter() - start\n",
    "\n",
    "\n",
    "# Predicts classes for unseen test videos.\n",
    "\n",
    "start = time.perf_counter();\n",
    "rf_test_pred = best_rf.predict(X_test_pca_rf);\n",
    "rf_test_time = time.perf_counter() - start;\n",
    "\n",
    "\n",
    "# Reports final official performance.\n",
    "print(\"Final RF Test Accuracy:\", accuracy_score(y_test, rf_test_pred));\n",
    "print(classification_report(y_test, rf_test_pred));\n",
    "\n",
    "\n",
    "# Visualizes correct and incorrect predictions.\n",
    "save_path = \"../results/confusion_matrices/classical/rf_confusion_matrix.png\";\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test, rf_test_pred);\n",
    "plt.title(\"Random Forest: Final Test Confusion Matrix\");\n",
    "plt.tight_layout();\n",
    "plt.savefig(save_path, dpi=300);\n",
    "plt.show();\n",
    "plt.close();\n",
    "\n",
    "np.save(\"../results/stats_classical/rf_train_time.npy\", rf_train_time);\n",
    "np.save(\"../results/stats_classical/rf_test_time.npy\", rf_test_time);\n",
    "\n",
    "rf_acc = accuracy_score(y_test, rf_test_pred);\n",
    "rf_f1  = f1_score(y_test, rf_test_pred, average=\"macro\");\n",
    "\n",
    "np.save(\"../results/stats_classical/rf_accuracy.npy\", rf_acc);\n",
    "np.save(\"../results/stats_classical/rf_f1.npy\", rf_f1);\n",
    "\n",
    "# Save RF-specific test features for comparative analysis (memory, efficiency etc.)\n",
    "np.save(\"../results/stats_classical/X_test_rf.npy\", X_test_pca_rf);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94442ae5",
   "metadata": {},
   "source": [
    "Random Forest: Error Analysis – Identify failure cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices where the model prediction is wrong i.e., predicted label is not equal to true label.\n",
    "wrong_idx = np.where(y_test != rf_test_pred)[0];\n",
    "\n",
    "# Print total number of misclassified test samples\n",
    "print(\"\\nNumber of misclassified test videos:\", len(wrong_idx));\n",
    "\n",
    "# Display details of a few wrong predictions (first 5 only)\n",
    "# This helps in manually inspecting failure cases\n",
    "for i in wrong_idx[:5]:\n",
    "\n",
    "    # Print the video file path\n",
    "    print(\"Video:\", test_videos[i]);\n",
    "\n",
    "    # Print the true class label and the model's predicted label\n",
    "    print(\"True Label:\", y_test[i], \"Predicted Label:\", rf_test_pred[i]);\n",
    "\n",
    "    # Print a separator line for better readability\n",
    "    print(\"-\" * 50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191ea75",
   "metadata": {},
   "source": [
    "Feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest computes how important each feature was for splitting decisions.\n",
    "# Higher value → more discriminative feature.\n",
    "importances = best_rf.feature_importances_;\n",
    "\n",
    "# Sorts features by importance and selects top 25 from the features we get after PCA.\n",
    "indices = np.argsort(importances)[::-1][:25];\n",
    "\n",
    "# Plots most influential features.\n",
    "# This visually demonstrates:\n",
    "    # which features contributed most\n",
    "    # why ensemble learning works well\n",
    "\n",
    "save_path = \"../results/feature_visualizations/rf_top25_feature_importance.png\";\n",
    "\n",
    "plt.figure(figsize=(10,5));\n",
    "plt.bar(range(len(indices)), importances[indices]);\n",
    "plt.xticks(range(len(indices)), indices, rotation=90);\n",
    "plt.title(\"POST PCA: Top 25 Feature Importances (Random Forest)\");\n",
    "plt.ylabel(\"Importance score\");\n",
    "plt.xlabel(\"Feature index\");\n",
    "plt.tight_layout();\n",
    "plt.savefig(save_path, dpi=300);\n",
    "plt.show();\n",
    "plt.close();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568934b9",
   "metadata": {},
   "source": [
    "k-Nearest Neighbors (k-NN)\n",
    "- Distance metric comparison\n",
    "- Optimal k value selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4610cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline k-NN model\n",
    "# This step builds a pipeline:\n",
    "# Step 1 → StandardScaler()\n",
    "# makes each feature zero-mean and unit-variance\n",
    "    # Step 2 → KNeighborsClassifier(...)\n",
    "# n_neighbors=5 → look at 5 nearest points\n",
    "# metric=\"euclidean\" → straight-line distance\n",
    "\n",
    "knn_base = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=10)),      # very strong compression\n",
    "    (\"knn\", KNeighborsClassifier(\n",
    "        n_neighbors=15,                 # large neighborhood\n",
    "        weights=\"uniform\",              # no distance bias\n",
    "        metric=\"manhattan\"               # weaker than euclidean here\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# Stores all training feature vectors and their labels.\n",
    "knn_base.fit(X_train, y_train);\n",
    "\n",
    "# For each validation video:\n",
    "# compute distance to all training videos\n",
    "# find 5 nearest\n",
    "# majority vote → predicted class\n",
    "\n",
    "val_pred_knn = knn_base.predict(X_val);\n",
    "\n",
    "# Evaluates baseline k-NN performance.\n",
    "# This tells how well k=5 with Euclidean distance works.\n",
    "\n",
    "print(\"Baseline kNN Validation Accuracy:\", accuracy_score(y_val, val_pred_knn));\n",
    "print(classification_report(y_val, val_pred_knn));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4609633",
   "metadata": {},
   "source": [
    "K-NN: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daa93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the search space:\n",
    "    # different values of k\n",
    "    # different distance metrics\n",
    "    # This allows the system to find the best neighborhood size and distance definition.\n",
    "\n",
    "param_grid = {\n",
    "    \"knn__n_neighbors\": [11,15,21,25],\n",
    "}\n",
    "\n",
    "# Creates a grid search object.\n",
    "    # build many k-NN models\n",
    "    # test all parameter combinations\n",
    "    # use 5-fold cross-validation\n",
    "    # measure average accuracy\n",
    "    # run in parallel\n",
    "\n",
    "grid_knn = GridSearchCV(\n",
    "    Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\", PCA(n_components=10)),\n",
    "        (\"knn\", KNeighborsClassifier(metric=\"manhattan\"))\n",
    "    ]),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=4,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_knn.fit(X_train, y_train);\n",
    "\n",
    "# Shows:\n",
    "    # optimal k\n",
    "    #optimal distance metric\n",
    "    # best cross-validated accuracy\n",
    "\n",
    "print(\"Best kNN Parameters:\", grid_knn.best_params_);\n",
    "print(\"Best CV Accuracy:\", grid_knn.best_score_);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf16c7c",
   "metadata": {},
   "source": [
    "Tuned k-NN on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ddd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the best tuned k-NN pipeline (Scaler + PCA + kNN);\n",
    "best_knn = grid_knn.best_estimator_;\n",
    "\n",
    "# Tests tuned k-NN on validation data.\n",
    "val_pred_best = best_knn.predict(X_val);\n",
    "\n",
    "# Checks whether tuning improved performance.\n",
    "print(\"Tuned kNN Validation Accuracy:\", accuracy_score(y_val, val_pred_best));\n",
    "print(classification_report(y_val, val_pred_best));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b4f347",
   "metadata": {},
   "source": [
    "k-NN: Final training and test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merges training and validation sets.\n",
    "X_final = np.vstack([X_train, X_val]);\n",
    "y_final = np.hstack([y_train, y_val]);\n",
    "\n",
    "# Stores all final training feature vectors.\n",
    "start = time.perf_counter();\n",
    "best_knn.fit(X_final, y_final);\n",
    "knn_train_time = time.perf_counter() - start;\n",
    "\n",
    "\n",
    "\n",
    "# Predicts labels for unseen test videos.\n",
    "start = time.perf_counter();\n",
    "knn_test_pred = best_knn.predict(X_test);\n",
    "knn_test_time = time.perf_counter() - start;\n",
    "\n",
    "# Outputs official final results.\n",
    "print(\"Final kNN Test Accuracy:\", accuracy_score(y_test, knn_test_pred));\n",
    "print(classification_report(y_test, knn_test_pred));\n",
    "\n",
    "\n",
    "# Shows:\n",
    "    # correct classifications (diagonal)\n",
    "    # confusions between actions\n",
    "\n",
    "os.makedirs(\"../results/confusion_matrices\", exist_ok=True);\n",
    "save_path = \"../results/confusion_matrices/knn_confusion_matrix.png\";\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, knn_test_pred);\n",
    "plt.title(\"k-NN: Final Test Confusion Matrix\");\n",
    "plt.tight_layout();\n",
    "plt.savefig(save_path, dpi=300);\n",
    "plt.show();\n",
    "plt.close();\n",
    "\n",
    "np.save(\"../results/stats_classical/knn_train_time.npy\", knn_train_time);\n",
    "np.save(\"../results/stats_classical/knn_test_time.npy\", knn_test_time);\n",
    "\n",
    "\n",
    "# Saving performance metrics (for comparative analysis)\n",
    "knn_acc = accuracy_score(y_test, knn_test_pred);\n",
    "knn_f1  = f1_score(y_test, knn_test_pred, average=\"macro\");\n",
    "\n",
    "np.save(\"../results/stats_classical/knn_accuracy.npy\", knn_acc);\n",
    "np.save(\"../results/stats_classical/knn_f1.npy\", knn_f1);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f98075",
   "metadata": {},
   "source": [
    "k-NN: Error Analysis – Identify failure cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a301fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_idx = np.where(y_test != knn_test_pred)[0];\n",
    "\n",
    "print(\"\\nNumber of misclassified test videos:\", len(wrong_idx));\n",
    "print(\"-\" * 50);\n",
    "for i in wrong_idx:\n",
    "    print(\"Video:\", test_videos[i]);\n",
    "    print(\"True Label:\", y_test[i], \"Predicted Label:\", knn_test_pred[i]);\n",
    "    print(\"-\" * 50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aebc62",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "- L1/L2 regularization\n",
    "- Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af6a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a three-stage machine learning pipeline:\n",
    "# StandardScaler()\n",
    "    # Scales all features to the same range.\n",
    "    # Prevents features with large values from dominating training.\n",
    "\n",
    "# PCA(n_components=50)\n",
    "    # Reduces thousands of handcrafted features into 50 most informative components.\n",
    "    # Removes noise, correlation, and reduces overfitting.\n",
    "\n",
    "# LogisticRegression(...)\n",
    "    # C=0.005 → strong regularization (keeps model simple, avoids memorization).\n",
    "    # solver=\"lbfgs\" → fast optimizer for multiclass problems.\n",
    "    # max_iter=2000 → ensures proper convergence.\n",
    "\n",
    "# This pipeline ensures:\n",
    "    # normalization → dimensionality reduction → classification.\n",
    "\n",
    "logreg = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=8, random_state=42)),\n",
    "    (\"logreg\", LogisticRegression(\n",
    "        C=0.001,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=2000\n",
    "    ))\n",
    "]);\n",
    "\n",
    "\n",
    "# -------- Train on TRAIN, evaluate on VAL --------\n",
    "#The pipeline learns:\n",
    "    # scaling parameters\n",
    "    # PCA directions\n",
    "    # logistic regression weights from the training data only.\n",
    "\n",
    "logreg.fit(X_train, y_train);\n",
    "\n",
    "# Uses the trained model to predict labels of validation videos.\n",
    "# Validation data is unseen during training.\n",
    "\n",
    "val_pred = logreg.predict(X_val);\n",
    "\n",
    "# accuracy_score → overall correctness.\n",
    "# classification_report → shows precision, recall, and F1-score for each action class.\n",
    "# Helps judge whether the model generalizes well.\n",
    "\n",
    "print(\"Logistic Regression Validation Accuracy:\", accuracy_score(y_val, val_pred));\n",
    "print(classification_report(y_val, val_pred));\n",
    "\n",
    "\n",
    "# -------- Final training (TRAIN + VAL) --------\n",
    "# Combines training and validation data.\n",
    "# Gives the final model more samples to learn from before final testing.\n",
    "\n",
    "X_final = np.vstack([X_train, X_val]);\n",
    "y_final = np.hstack([y_train, y_val]);\n",
    "\n",
    "# Retrains the model using all available labeled data (except test set).\n",
    "# Produces the strongest possible final classifier.\n",
    "start = time.perf_counter();\n",
    "logreg.fit(X_final, y_final);\n",
    "logreg_train_time = time.perf_counter() - start;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc3ba9",
   "metadata": {},
   "source": [
    "Logistic Regression: Final TEST evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f138248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predicts actions of completely unseen test videos.\n",
    "# This represents the true performance of the model.\n",
    "start = time.perf_counter();\n",
    "logreg_test_pred = logreg.predict(X_test);\n",
    "logreg_test_time = time.perf_counter() - start\n",
    "\n",
    "# Measures how well the final model generalizes.\n",
    "# These are the results you report.\n",
    "\n",
    "print(\"Final Logistic Regression Test Accuracy:\", accuracy_score(y_test, logreg_test_pred));\n",
    "print(classification_report(y_test, logreg_test_pred));\n",
    "\n",
    "# Displays how many samples were correctly or incorrectly classified.\n",
    "# Helps analyze which actions are confused with each other.\n",
    "os.makedirs(\"../results/confusion_matrices\", exist_ok=True);\n",
    "save_path = \"../results/confusion_matrices/logreg_confusion_matrix.png\"\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, logreg_test_pred);\n",
    "plt.title(\"Logistic Regression – Final Test Confusion Matrix\");\n",
    "plt.tight_layout();\n",
    "plt.savefig(save_path, dpi=300);\n",
    "plt.show();\n",
    "plt.close();\n",
    "\n",
    "\n",
    "np.save(\"../results/stats_classical/logreg_train_time.npy\", logreg_train_time);\n",
    "np.save(\"../results/stats_classical/logreg_test_time.npy\", logreg_test_time);\n",
    "\n",
    "# Saving performance metrics (for comparative analysis)\n",
    "logreg_acc = accuracy_score(y_test, logreg_test_pred);\n",
    "logreg_f1  = f1_score(y_test, logreg_test_pred, average=\"macro\");\n",
    "\n",
    "np.save(\"../results/stats_classical/logreg_accuracy.npy\", logreg_acc);\n",
    "np.save(\"../results/stats_classical/logreg_f1.npy\", logreg_f1);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c9d94",
   "metadata": {},
   "source": [
    "Logistic Regression: Error Analysis – Identify failure cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e86674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wrong_idx = np.where(y_test != logreg_test_pred)[0];\n",
    "\n",
    "print(\"\\nNumber of misclassified test videos:\", len(wrong_idx));\n",
    "\n",
    "for i in wrong_idx:\n",
    "    print(\"Video:\", test_videos[i]);\n",
    "    print(\"True Label:\", y_test[i], \"Predicted Label:\", logreg_test_pred[i]);\n",
    "    print(\"-\" * 50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74818b30",
   "metadata": {},
   "source": [
    "Gradient Boosting (XGBoost or LightGBM)\n",
    "- Tree-based ensemble learning\n",
    "- Feature importance ranking\n",
    "\n",
    "\n",
    "Train Gradient Boosting on TRAIN set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02764e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(\n",
    "\n",
    "    # Number of boosting stages (trees).\n",
    "    # Fewer trees reduce model complexity and overfitting.\n",
    "    n_estimators=80,\n",
    "\n",
    "    # Shrinks the contribution of each tree.\n",
    "    # Smaller values make learning slower but more stable\n",
    "    # and improve generalization.\n",
    "    learning_rate=0.05,\n",
    "\n",
    "    # Maximum depth of each individual decision tree.\n",
    "    # Shallow trees act as weak learners and prevent memorization.\n",
    "    max_depth=2,\n",
    "\n",
    "    # Fraction of training samples used to fit each tree.\n",
    "    # Using less than 100% introduces randomness and acts as\n",
    "    # regularization (stochastic gradient boosting).\n",
    "    subsample=0.8,\n",
    "\n",
    "    # Fixes random behavior for reproducibility.\n",
    "    random_state=42\n",
    ");\n",
    "\n",
    "gb.fit(X_train, y_train);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293512e",
   "metadata": {},
   "source": [
    "Gradient Boosting: Validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5922182",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_gb = gb.predict(X_val);\n",
    "\n",
    "print(\"Gradient Boosting Validation Accuracy:\", accuracy_score(y_val, val_pred_gb));\n",
    "print(classification_report(y_val, val_pred_gb));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2175b73",
   "metadata": {},
   "source": [
    "Gradient Boosting: Final training (TRAIN + VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69828af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = np.vstack([X_train, X_val]);\n",
    "y_final = np.hstack([y_train, y_val]);\n",
    "\n",
    "start = time.perf_counter();\n",
    "gb.fit(X_final, y_final);\n",
    "gb_train_time = time.perf_counter() - start;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717d041",
   "metadata": {},
   "source": [
    "Gradient Boosting: Final TEST evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281fb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter();\n",
    "gb_test_pred = gb.predict(X_test);\n",
    "gb_test_time = time.perf_counter() - start;\n",
    "print(\"Final Gradient Boosting Test Accuracy:\", accuracy_score(y_test, gb_test_pred));\n",
    "print(classification_report(y_test, gb_test_pred));\n",
    "\n",
    "os.makedirs(\"../results/confusion_matrices\", exist_ok=True);\n",
    "save_path = \"../results/confusion_matrices/gb_confusion_matrix.png\";\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, gb_test_pred);\n",
    "plt.title(\"Gradient Boosting – Final Test Confusion Matrix\");\n",
    "plt.tight_layout();\n",
    "plt.savefig(save_path, dpi=300);\n",
    "plt.show();\n",
    "plt.close();\n",
    "\n",
    "\n",
    "np.save(\"../results/stats_classical/gb_train_time.npy\", gb_train_time);\n",
    "np.save(\"../results/stats_classical/gb_test_time.npy\", gb_test_time);\n",
    "\n",
    "gb_acc = accuracy_score(y_test, gb_test_pred);\n",
    "gb_f1  = f1_score(y_test, gb_test_pred, average=\"macro\");\n",
    "\n",
    "np.save(\"../results/stats_classical/gb_accuracy.npy\", gb_acc);\n",
    "np.save(\"../results/stats_classical/gb_f1.npy\", gb_f1);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52260412",
   "metadata": {},
   "source": [
    "Gradient Boosting: Error Analysis – Identify failure cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86af7e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wrong_idx = np.where(y_test != gb_test_pred)[0]\n",
    "\n",
    "print(\"\\nNumber of misclassified test videos:\", len(wrong_idx))\n",
    "\n",
    "for i in wrong_idx[:5]:\n",
    "    print(\"Video:\", test_videos[i])\n",
    "    print(\"True Label:\", y_test[i], \"Predicted Label:\", gb_test_pred[i])\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84511095",
   "metadata": {},
   "source": [
    "Gradient Boosting: Feature importance ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0fabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = gb.feature_importances_;\n",
    "\n",
    "# Top 20 important features\n",
    "indices = np.argsort(importances)[-20:][::-1];\n",
    "plt.figure(figsize=(10,5));\n",
    "plt.bar(range(20), importances[indices]);\n",
    "plt.xticks(range(20), indices, rotation=90);\n",
    "plt.title(\"Top 20 Feature Importances (Gradient Boosting)\");\n",
    "plt.xlabel(\"Feature Index\");\n",
    "plt.ylabel(\"Importance Score\");\n",
    "plt.tight_layout();\n",
    "plt.savefig(\"../results/feature_visualizations/gb_top20_feature_importance.png\", dpi=300);\n",
    "plt.show();\n",
    "plt.close();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b38c8d4",
   "metadata": {},
   "source": [
    "At this stage, following Classical Machine Learning models have been implemented:\n",
    "1. Support Vector Machine (SVM)\n",
    "2. Random Forest\n",
    "3. k Nearest Neighbor (k-NN)\n",
    "4. Logistic Regression\n",
    "5. Gradient Boosting\n",
    "\n",
    "Next step:\n",
    "\n",
    "### Saving the final results of all the implemented models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4c3815",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../results/final_classical_X_test.npy\", X_test);\n",
    "np.save(\"../results/final_classical_y_test.npy\", y_test);\n",
    "\n",
    "np.save(\"../results/stats_classical/svm_test_pred.npy\", svm_test_pred);\n",
    "np.save(\"../results/stats_classical/rf_test_pred.npy\", rf_test_pred);\n",
    "np.save(\"../results/stats_classical/knn_test_pred.npy\", knn_test_pred);\n",
    "np.save(\"../results/stats_classical/logreg_test_pred.npy\", logreg_test_pred);\n",
    "np.save(\"../results/stats_classical/gb_test_pred.npy\", gb_test_pred);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e19aa0",
   "metadata": {},
   "source": [
    "Saving the trained models on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5372c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving trained SVM:\", joblib.dump(best_svm, \"../results/saved_models/classical/svm_trained_model.joblib\"));\n",
    "print(\"Saving trained Random Forest:\",joblib.dump(best_rf, \"../results/saved_models/classical/rf_trained_model.joblib\"));\n",
    "print(\"Saving trained KNN:\", joblib.dump(best_knn, \"../results/saved_models/classical/knn_trained_model.joblib\"));\n",
    "print(\"Saving trained Logistic Regression:\",joblib.dump(logreg, \"../results/saved_models/classical/logreg_trained_model.joblib\"));\n",
    "print(\"Saving trained Gradient Boosting:\",joblib.dump(gb, \"../results/saved_models/classical/gb_trained_model.joblib\"));\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
