{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b952f73",
   "metadata": {},
   "source": [
    "### Student information:\n",
    "- Name: Vineet Kumar\n",
    "- Roll No.: 2024AC05100\n",
    "- Assignment-1: Video Classification\n",
    "\n",
    "### Assignment objective: \n",
    "\n",
    "Using the customized dataset \"UCF-101\" to perform \"Action recognition\" for 3 different classes to predict the correct video class (Video Classification) using Classical Machine learning and Deep Learning Models.\n",
    "\n",
    "### Task: Action recognition:\n",
    "1. Using Classical Machine Learning models.\n",
    "2. Using Deep Learning Models\n",
    "\n",
    "### This python notebook file contains the code for following tasks:\n",
    "1. Loading the dataset for 3 classes:\n",
    "    - Class-1: PullUps\n",
    "    - Class-2: Punch\n",
    "    - Class-3: PullUps\n",
    "    - The dataset is split into Train, Test and Validation.\n",
    "    - For each category there is a separate CSV file.\n",
    "2. Implementation of Deep Learning Learning models like 2D-CNN and 3D-CNN etc.\n",
    "3. Model hyperparameter tuning\n",
    "4. Performance evaluation and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f3bfc",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed35ee",
   "metadata": {},
   "source": [
    "1. Installing required modules\n",
    "\n",
    "- Before running this notebook, install dependencies using:\n",
    "> `pip install -r requirements.txt`\n",
    "\n",
    "- (Ignore if already done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7160dd9d",
   "metadata": {},
   "source": [
    "2. importing modules and basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e45f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os;\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from data_loader import VideoDataset2D\n",
    "from models import ResNet18Temporal\n",
    "from utils import train_one_epoch, eval_one_epoch, EarlyStopping\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import time\n",
    "import random\n",
    "import psutil\n",
    "from data_loader import VideoDataset3D\n",
    "from models import ResNet3D\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4d229",
   "metadata": {},
   "source": [
    "3. Reproducibility and Random Seed Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93221d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84738b9d",
   "metadata": {},
   "source": [
    "4. Creating required directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30610440",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../results\", exist_ok=True);\n",
    "os.makedirs(\"../results/confusion_matrices\", exist_ok=True);\n",
    "os.makedirs(\"../results/confusion_matrices/deep_learning\", exist_ok=True);\n",
    "os.makedirs(\"../results/performance_plots\", exist_ok=True);\n",
    "os.makedirs(\"../results/performance_plots/deep_learning\", exist_ok=True);\n",
    "os.makedirs(\"../results/feature_visualizations\", exist_ok=True);\n",
    "os.makedirs(\"../results/saved_models\", exist_ok=True);\n",
    "os.makedirs(\"../results/saved_models/deep_learning\", exist_ok=True);\n",
    "os.makedirs(\"../results/stats_deep_learning\", exist_ok=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d34c5",
   "metadata": {},
   "source": [
    "5. Directory path for saving trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9fb422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAVE_DIR = \"../results/saved_models/deep_learning\";\n",
    "os.makedirs(SAVE_DIR, exist_ok=True);\n",
    "\n",
    "MODEL_2D_PATH = os.path.join(SAVE_DIR, \"best_2d-cnn.pth\");\n",
    "MODEL_3D_PATH = os.path.join(SAVE_DIR, \"best_3d-cnn.pth\");\n",
    "\n",
    "\n",
    "STATS_2D_PATH = \"../results/stats_deep_learning/stats_2d.json\";\n",
    "STATS_3D_PATH = \"../results/stats_deep_learning/stats_3d.json\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a35d5",
   "metadata": {},
   "source": [
    "6. Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabb8d19",
   "metadata": {},
   "source": [
    "7. Dataset & loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35117296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\"PullUps\": 0, \"Punch\": 1, \"PushUps\": 2}\n",
    "num_classes = len(class_map)\n",
    "\n",
    "dataset_root = \"../dataset_info/dataset\"\n",
    "\n",
    "train_ds_2d = VideoDataset2D(\"../dataset_info/dataset/splits/train.csv\",\n",
    "                         dataset_root, class_map,\n",
    "                         num_frames=20, transform=train_tfms, train=True)\n",
    "\n",
    "val_ds_2d = VideoDataset2D(\"../dataset_info/dataset/splits/val.csv\",\n",
    "                       dataset_root, class_map,\n",
    "                       num_frames=20, transform=val_tfms, train=False)\n",
    "\n",
    "test_ds_2d = VideoDataset2D(\"../dataset_info/dataset/splits/test.csv\",\n",
    "                        dataset_root, class_map,\n",
    "                        num_frames=20, transform=val_tfms, train=False)\n",
    "\n",
    "train_loader_2d = DataLoader(train_ds_2d, batch_size=4, shuffle=True, num_workers=4)\n",
    "val_loader_2d   = DataLoader(val_ds_2d, batch_size=4, shuffle=False, num_workers=4)\n",
    "test_loader_2d  = DataLoader(test_ds_2d, batch_size=4, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25014773",
   "metadata": {},
   "source": [
    "### Deep Learning Algorithms\n",
    "\n",
    "Implementing following Deep Learning Models\n",
    "- 2D-CNN - with temporal pooling (ResNet18 pretrained on ImageNet)\n",
    "- 3D-CNN - pre-trained 3D ResNet-18 (R3D-18), which is an I3D-style 3D CNN architecture\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "1. 2D CNN with Temporal Pooling\n",
    " - Use pre-trained 2D CNN (ResNet-18, ResNet-50, or EfficientNet-B0) for frame-level features\n",
    " - Extract features from sampled frames (e.g., 16-32 frames per video)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a8829c",
   "metadata": {},
   "source": [
    "2D CNN: Model, optimizer and scheduler selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af2964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_2d = ResNet18Temporal(num_classes=num_classes, pooling=\"avg\", dropout=0.5).to(device)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.9)\n",
    "\n",
    "# Using Adam as optimizer\n",
    "optimizer = torch.optim.Adam(model_2d.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "# Learning rate scheduling\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "# Early stopping\n",
    "early_stop = EarlyStopping(patience=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7440e3bb",
   "metadata": {},
   "source": [
    "2D CNN: Training loop (with early stopping) \n",
    "- This will take around 30 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b9035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "EPOCHS = 12\n",
    "best_val_acc = 0.0\n",
    "\n",
    "train_losses_2d = []\n",
    "val_losses_2d   = []\n",
    "train_accs_2d   = []\n",
    "val_accs_2d     = []\n",
    "\n",
    "# ================= LOAD IF EXISTS =================\n",
    "if os.path.exists(MODEL_2D_PATH):\n",
    "\n",
    "    print(\"Found saved 2D model. Loading...\")\n",
    "\n",
    "    model_2d.load_state_dict(torch.load(MODEL_2D_PATH, map_location=device))\n",
    "    model_2d.to(device)\n",
    "    model_2d.eval()\n",
    "\n",
    "    if os.path.exists(STATS_2D_PATH):\n",
    "        with open(STATS_2D_PATH, \"r\") as f:\n",
    "            stats = json.load(f)\n",
    "\n",
    "        # ---- SAFE LOAD (no KeyError) ----\n",
    "        train_losses_2d = stats.get(\"train_losses\", [])\n",
    "        val_losses_2d   = stats.get(\"val_losses\", [])\n",
    "        train_accs_2d   = stats.get(\"train_accs\", [])\n",
    "        val_accs_2d     = stats.get(\"val_accs\", [])\n",
    "        best_val_acc    = max(val_accs_2d) if len(val_accs_2d) > 0 else 0.0\n",
    "\n",
    "        print(\"Loaded training curves from stats_2d.json\")\n",
    "    else:\n",
    "        print(\"stats_2d.json not found → curves unavailable\")\n",
    "\n",
    "# ================= TRAIN IF NOT EXISTS =================\n",
    "else:\n",
    "    print(\"No saved 2D model found. Starting training...\")\n",
    "\n",
    "    for epoch in trange(EPOCHS, desc=\"2D CNN Epochs\"):\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model_2d, train_loader_2d, optimizer, criterion, device)\n",
    "\n",
    "        val_loss, val_acc = eval_one_epoch(\n",
    "            model_2d, val_loader_2d, criterion, device)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(\n",
    "            f\"\\nEpoch [{epoch+1}/{EPOCHS}] \"\n",
    "            f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} || \"\n",
    "            f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        train_losses_2d.append(train_loss)\n",
    "        val_losses_2d.append(val_loss)\n",
    "        train_accs_2d.append(train_acc)\n",
    "        val_accs_2d.append(val_acc)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model_2d.state_dict(), MODEL_2D_PATH)\n",
    "\n",
    "        early_stop(val_loss)\n",
    "        if early_stop.stop:\n",
    "            print(\"⏹ Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    print(\"2D CNN Training complete. Best Val Acc:\", best_val_acc)\n",
    "\n",
    "    # ================= SAVE TRAINING CURVES =================\n",
    "    stats_2d = {\n",
    "        \"train_losses\": train_losses_2d,\n",
    "        \"val_losses\": val_losses_2d,\n",
    "        \"train_accs\": train_accs_2d,\n",
    "        \"val_accs\": val_accs_2d,\n",
    "        \"best_val_acc\": best_val_acc\n",
    "    }\n",
    "\n",
    "    with open(STATS_2D_PATH, \"w\") as f:\n",
    "        json.dump(stats_2d, f, indent=4)\n",
    "\n",
    "    print(\"Training curves saved to stats_2d.json\")\n",
    "\n",
    "end_time = time.time()\n",
    "train_time_2d = end_time - start_time\n",
    "\n",
    "np.save(\"../results/stats_deep_learning/train_time_2d.npy\", train_time_2d)\n",
    "print(\"2D CNN Training Time (seconds):\", train_time_2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527f589",
   "metadata": {},
   "source": [
    "2D CNN: Loading best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best saved model\n",
    "model_2d.load_state_dict(torch.load(MODEL_2D_PATH, map_location=device))\n",
    "model_2d.eval()\n",
    "\n",
    "print(\"Model loaded successfully\")\n",
    "print(\"Backbone: ResNet-18 (ImageNet pretrained)\")\n",
    "print(\"Input: fixed-length RGB frame sequences (224x224)\")\n",
    "print(\"Temporal aggregation: Temporal pooling\")\n",
    "print(\"Classifier head: Dropout → Fully Connected (3 classes)\")\n",
    "\n",
    "print(\"Trainable parameters:\",\n",
    "      sum(p.numel() for p in model_2d.parameters() if p.requires_grad))\n",
    "\n",
    "print(\"Total parameters:\",\n",
    "      sum(p.numel() for p in model_2d.parameters()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57976c1",
   "metadata": {},
   "source": [
    "2D CNN: Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e43245",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_2d = []\n",
    "all_labels_2d = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader_2d:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model_2d(x)\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "\n",
    "        all_preds_2d.extend(preds.cpu().numpy())\n",
    "        all_labels_2d.extend(y.cpu().numpy())\n",
    "\n",
    "all_preds_2d = np.array(all_preds_2d)\n",
    "all_labels_2d = np.array(all_labels_2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f00cf3",
   "metadata": {},
   "source": [
    "2D CNN: Evaluation metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2D CNN Test Accuracy:\", accuracy_score(all_labels_2d, all_preds_2d))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels_2d, all_preds_2d, target_names=list(class_map.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8309ad2",
   "metadata": {},
   "source": [
    "2D CNN: Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_2d = confusion_matrix(all_labels_2d, all_preds_2d)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(conf_matrix_2d, annot=True, fmt=\"d\",\n",
    "            xticklabels=class_map.keys(),\n",
    "            yticklabels=class_map.keys(),\n",
    "            cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix – 2D CNN (ResNet-18)\")\n",
    "plt.savefig(\"../results/confusion_matrices/deep_learning/2d_resnet18_confusion_matrix.png\")\n",
    "plt.show()\n",
    "plt.close();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a6f57",
   "metadata": {},
   "source": [
    "2D CNN: Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097b2e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Train losses:\", train_losses_2d)\n",
    "#print(\"Val losses:\", val_losses_2d)\n",
    "#print(\"Train accs:\", train_accs_2d)\n",
    "#print(\"Val accs:\", val_accs_2d)\n",
    "\n",
    "#print(len(train_losses_2d), len(val_losses_2d), len(train_accs_2d), len(val_accs_2d))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses_2d, label=\"Train Loss\")\n",
    "plt.plot(val_losses_2d, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve – 2D CNN\")\n",
    "plt.legend()\n",
    "plt.savefig(\"../results/performance_plots/deep_learning/2d_cnn_loss_curve.png\", dpi=300, bbox_inches=\"tight\");\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_accs_2d, label=\"Train Accuracy\")\n",
    "plt.plot(val_accs_2d, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Curve – 2D CNN\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig(\"../results/performance_plots/deep_learning/2d_cnn_accuracy_curve.png\", dpi=300, bbox_inches=\"tight\");\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf85435e",
   "metadata": {},
   "source": [
    "2D CNN: Inference time (computational analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b64fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(test_loader_2d));\n",
    "x = x.to(device);\n",
    "\n",
    "start = time.time();\n",
    "with torch.no_grad():\n",
    "    _ = model_2d(x)\n",
    "end = time.time();\n",
    "\n",
    "inf_time_2d = (end - start) / x.size(0)  ;\n",
    "\n",
    "np.save(\"../results/stats_deep_learning/inf_time_2d.npy\", inf_time_2d);\n",
    "print(\"Inference time per batch (seconds):\", end-start);\n",
    "print(\"Approx inference time per video (seconds)):\", inf_time_2d);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613172e3",
   "metadata": {},
   "source": [
    "2D CNN: Model size (parameter count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params_2d = sum(p.numel() for p in model_2d.parameters())\n",
    "trainable_params_2d = sum(p.numel() for p in model_2d.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Total parameters:\", total_params_2d)\n",
    "print(\"Trainable parameters:\", trainable_params_2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e96057",
   "metadata": {},
   "source": [
    "2D CNN: Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc0d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_idx = np.where(all_preds_2d != all_labels_2d)[0]\n",
    "print(\"Wrong predictions:\", len(wrong_idx))\n",
    "\n",
    "if len(wrong_idx) > 0:\n",
    "    for i in wrong_idx[:5]:\n",
    "\n",
    "        row = test_ds_2d.data.iloc[i]   # CSV row\n",
    "        video_path = row[\"clip_path\"]\n",
    "\n",
    "        true_label = list(class_map.keys())[all_labels_2d[i]]\n",
    "        pred_label = list(class_map.keys())[all_preds_2d[i]]\n",
    "\n",
    "        print(\"Video:\", video_path)\n",
    "        print(\"True:\", true_label, \"| Pred:\", pred_label)\n",
    "        print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643d2833",
   "metadata": {},
   "source": [
    "2D CNN: File size on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = MODEL_2D_PATH\n",
    "file_size_mb_2d = os.path.getsize(model_path) / (1024 * 1024)\n",
    "np.save(\"../results/stats_deep_learning/model_size_2d.npy\", file_size_mb_2d)\n",
    "print(\"Saved model file size: {:.2f} MB\".format(file_size_mb_2d))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0f0c6",
   "metadata": {},
   "source": [
    "2D CNN: Memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2331b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Torch CUDA version:\", torch.version.cuda)\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU memory allocated:\",\n",
    "          torch.cuda.memory_allocated() / (1024**2), \"MB\")\n",
    "    print(\"GPU memory reserved:\",\n",
    "          torch.cuda.memory_reserved() / (1024**2), \"MB\")\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "ram_mb = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "print(\"Current RAM usage:\", ram_mb, \"MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38208fcc",
   "metadata": {},
   "source": [
    "2D CNN: Saving the results for comparitive analyisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f160c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc2d = accuracy_score(all_labels_2d, all_preds_2d)\n",
    "p2d, r2d, f12d, _ = precision_recall_fscore_support(all_labels_2d, all_preds_2d, average=\"macro\")\n",
    "\n",
    "stats_2d = {\n",
    "    \"accuracy\": acc2d,\n",
    "    \"precision\": p2d,\n",
    "    \"recall\": r2d,\n",
    "    \"f1\": f12d,\n",
    "    \"inference_time\": inf_time_2d,\n",
    "    \"params\": total_params_2d,\n",
    "    \"model_size_mb\": file_size_mb_2d\n",
    "}\n",
    "\n",
    "with open(\"../results/stats_deep_learning/stats_2d.json\", \"w\") as f:\n",
    "    json.dump(stats_2d, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d142c7b6",
   "metadata": {},
   "source": [
    "2. 3D Convolutional Networks\n",
    "   \n",
    "- I3D Architecture\n",
    "- Implement or use pre-trained 3D CNN\n",
    "- Process video clips (e.g., 16 frames) as 3D volumes\n",
    "- Fine-tune on your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e8f547",
   "metadata": {},
   "source": [
    "3D CNN: Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc04593",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_3d = VideoDataset3D(\"../dataset_info/dataset/splits/train.csv\", dataset_root, class_map,\n",
    "                             num_frames=16, transform=train_tfms, train=True)\n",
    "\n",
    "val_ds_3d   = VideoDataset3D(\"../dataset_info/dataset/splits/val.csv\", dataset_root, class_map,\n",
    "                             num_frames=16, transform=val_tfms, train=False)\n",
    "\n",
    "test_ds_3d  = VideoDataset3D(\"../dataset_info/dataset/splits/test.csv\", dataset_root, class_map,\n",
    "                             num_frames=16, transform=val_tfms, train=False)\n",
    "\n",
    "train_loader_3d = DataLoader(train_ds_3d, batch_size=2, shuffle=True, num_workers=4)\n",
    "val_loader_3d   = DataLoader(val_ds_3d, batch_size=2, shuffle=False, num_workers=4)\n",
    "test_loader_3d  = DataLoader(test_ds_3d, batch_size=2, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "x3d, _ = next(iter(test_loader_3d))\n",
    "print(x3d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3ee5de",
   "metadata": {},
   "source": [
    "3D CNN: Model, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c88a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\"PullUps\": 0, \"Punch\": 1, \"PushUps\": 2}\n",
    "num_classes = len(class_map)\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_3d = ResNet3D(num_classes=num_classes, dropout=0.5, freeze_backbone=True).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.9)\n",
    "# Using Adam as optimizer\n",
    "optimizer = torch.optim.Adam(model_3d.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "# Learning rate scheduling\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "# Early stopping\n",
    "early_stop = EarlyStopping(patience=4)\n",
    "\n",
    "out = model_3d(x3d.to(device))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd9e34e",
   "metadata": {},
   "source": [
    "3D CNN: Training loop (With early stopping)\n",
    "- This will take around 30 minutes to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "EPOCHS = 12\n",
    "best_val_acc = 0.0\n",
    "\n",
    "train_losses_3d = []\n",
    "val_losses_3d   = []\n",
    "train_accs_3d   = []\n",
    "val_accs_3d     = []\n",
    "\n",
    "# ================= LOAD IF EXISTS =================\n",
    "if os.path.exists(MODEL_3D_PATH):\n",
    "\n",
    "    print(\"Found saved 3D model. Loading...\")\n",
    "\n",
    "    model_3d.load_state_dict(torch.load(MODEL_3D_PATH, map_location=device))\n",
    "    model_3d.to(device)\n",
    "    model_3d.eval()\n",
    "\n",
    "    if os.path.exists(STATS_3D_PATH):\n",
    "        with open(STATS_3D_PATH, \"r\") as f:\n",
    "            stats = json.load(f)\n",
    "\n",
    "        # ---- SAFE LOAD ----\n",
    "        train_losses_3d = stats.get(\"train_losses\", [])\n",
    "        val_losses_3d   = stats.get(\"val_losses\", [])\n",
    "        train_accs_3d   = stats.get(\"train_accs\", [])\n",
    "        val_accs_3d     = stats.get(\"val_accs\", [])\n",
    "        best_val_acc    = max(val_accs_3d) if len(val_accs_3d) > 0 else 0.0\n",
    "\n",
    "        print(\"Loaded training curves from stats_3d.json\")\n",
    "    else:\n",
    "        print(\"stats_3d.json not found → curves unavailable\")\n",
    "\n",
    "# ================= TRAIN IF NOT EXISTS =================\n",
    "else:\n",
    "    print(\"No saved 3D model found. Starting training...\")\n",
    "\n",
    "    for epoch in trange(EPOCHS, desc=\"3D CNN Epochs\"):\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model_3d, train_loader_3d, optimizer, criterion, device)\n",
    "\n",
    "        val_loss, val_acc = eval_one_epoch(\n",
    "            model_3d, val_loader_3d, criterion, device)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(\n",
    "            f\"\\nEpoch [{epoch+1}/{EPOCHS}] \"\n",
    "            f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} || \"\n",
    "            f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        train_losses_3d.append(train_loss)\n",
    "        val_losses_3d.append(val_loss)\n",
    "        train_accs_3d.append(train_acc)\n",
    "        val_accs_3d.append(val_acc)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model_3d.state_dict(), MODEL_3D_PATH)\n",
    "\n",
    "        early_stop(val_loss)\n",
    "        if early_stop.stop:\n",
    "            print(\"⏹ Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    print(\"3D CNN Training complete. Best Val Acc:\", best_val_acc)\n",
    "\n",
    "    # ================= SAVE TRAINING CURVES =================\n",
    "    stats_3d = {\n",
    "        \"train_losses\": train_losses_3d,\n",
    "        \"val_losses\": val_losses_3d,\n",
    "        \"train_accs\": train_accs_3d,\n",
    "        \"val_accs\": val_accs_3d,\n",
    "        \"best_val_acc\": best_val_acc\n",
    "    }\n",
    "\n",
    "    with open(STATS_3D_PATH, \"w\") as f:\n",
    "        json.dump(stats_3d, f, indent=4)\n",
    "\n",
    "    print(\"Training curves saved to stats_3d.json\")\n",
    "\n",
    "end_time = time.time()\n",
    "train_time_3d = end_time - start_time\n",
    "\n",
    "np.save(\"../results/stats_deep_learning/train_time_3d.npy\", train_time_3d)\n",
    "\n",
    "print(\"3D CNN Training Time (seconds):\", train_time_3d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cae0e8",
   "metadata": {},
   "source": [
    "3D CNN: Loading best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe8881",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3d.load_state_dict(torch.load(MODEL_3D_PATH, map_location=device))\n",
    "model_3d.eval()\n",
    "\n",
    "print(\"Model loaded successfully\")\n",
    "print(\"Backbone: ResNet-3D (r3d_18, Kinetics pretrained)\")\n",
    "print(\"Input: video clips as 3D volumes (C × T × H × W)\")\n",
    "print(\"Temporal modeling: 3D convolutions\")\n",
    "print(\"Classifier head: Dropout → Fully Connected (3 classes)\")\n",
    "\n",
    "print(\"Trainable parameters:\",\n",
    "      sum(p.numel() for p in model_3d.parameters() if p.requires_grad))\n",
    "\n",
    "print(\"Total parameters:\",\n",
    "      sum(p.numel() for p in model_3d.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc36773",
   "metadata": {},
   "source": [
    "3D CNN: Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0126b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3d.load_state_dict(torch.load(MODEL_3D_PATH, map_location=device))\n",
    "model_3d.eval()\n",
    "\n",
    "all_preds_3d, all_labels_3d = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader_3d:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model_3d(x)\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "\n",
    "        all_preds_3d.extend(preds.cpu().numpy())\n",
    "        all_labels_3d.extend(y.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17feed2f",
   "metadata": {},
   "source": [
    "3D CNN: Evaluation metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39963d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3D CNN Test Accuracy:\", accuracy_score(all_labels_3d, all_preds_3d))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels_3d, all_preds_3d, target_names=class_map.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6477dfa7",
   "metadata": {},
   "source": [
    "3D CNN: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2512c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_3d = confusion_matrix(all_labels_3d, all_preds_3d)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(conf_matrix_3d, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=class_map.keys(),\n",
    "            yticklabels=class_map.keys())\n",
    "plt.title(\"Confusion Matrix – 3D CNN\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.savefig(\"../results/confusion_matrices/deep_learning/3d_resnet3d_confusion_matrix.png\");\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f6d014",
   "metadata": {},
   "source": [
    "3D CNN: Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa43c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_losses_3d, label=\"Train Loss\")\n",
    "plt.plot(val_losses_3d, label=\"Val Loss\")\n",
    "plt.title(\"3D CNN Loss Curve\")\n",
    "plt.legend()\n",
    "plt.savefig(\"../results/performance_plots/deep_learning/3d_cnn_loss_curve.png\");\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_accs_3d, label=\"Train Acc\")\n",
    "plt.plot(val_accs_3d, label=\"Val Acc\")\n",
    "plt.title(\"3D CNN Accuracy Curve\")\n",
    "plt.legend()\n",
    "plt.savefig(\"../results/performance_plots/deep_learning/3d_cnn_accuracy_curve.png\");\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4f6610",
   "metadata": {},
   "source": [
    "3D CNN: Inference time (computational analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23150187",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(test_loader_3d))\n",
    "x = x.to(device)\n",
    "\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    _ = model_3d(x)\n",
    "end = time.time()\n",
    "\n",
    "inf_time_3d = (end - start) / x.size(0)  \n",
    "\n",
    "np.save(\"../results/stats_deep_learning/inf_time_3d.npy\", inf_time_3d)\n",
    "print(\"Inference time per batch:\", end-start)\n",
    "print(\"Approx inference time per video:\", inf_time_3d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bca80",
   "metadata": {},
   "source": [
    "3D CNN: Model size (parameter count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a565a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params_3d = sum(p.numel() for p in model_3d.parameters())\n",
    "trainable_params_3d = sum(p.numel() for p in model_3d.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"3D CNN Total parameters:\", total_params_3d)\n",
    "print(\"3D CNN Trainable parameters:\", trainable_params_3d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d000eca5",
   "metadata": {},
   "source": [
    "3D CNN: Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220211f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_idx = np.where(np.array(all_preds_3d) != np.array(all_labels_3d))[0]\n",
    "print(\"Total wrong predictions (3D CNN):\", len(wrong_idx))\n",
    "\n",
    "if len(wrong_idx) > 0:\n",
    "    #for i in wrong_idx[:5]:\n",
    "    for i in wrong_idx:    \n",
    "        row = test_ds_3d.data.iloc[i]\n",
    "        video_path = row[\"clip_path\"]\n",
    "\n",
    "        true_label = list(class_map.keys())[all_labels_3d[i]]\n",
    "        pred_label = list(class_map.keys())[all_preds_3d[i]]\n",
    "\n",
    "        print(\"Video:\", video_path)\n",
    "        print(\"True:\", true_label, \"| Pred:\", pred_label)\n",
    "        print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0625b",
   "metadata": {},
   "source": [
    "3D CNN: File size on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9480376",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = MODEL_3D_PATH\n",
    "file_size_mb_3d = os.path.getsize(model_path) / (1024 * 1024)\n",
    "np.save(\"../results/stats_deep_learning/model_size_3d.npy\", file_size_mb_3d)\n",
    "print(\"3D CNN Saved model size: {:.2f} MB\".format(file_size_mb_3d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd562d4",
   "metadata": {},
   "source": [
    "3D CNN: Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da9e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU memory allocated:\",\n",
    "          torch.cuda.memory_allocated() / (1024**2), \"MB\")\n",
    "    print(\"GPU memory reserved:\",\n",
    "          torch.cuda.memory_reserved() / (1024**2), \"MB\")\n",
    "else:\n",
    "    print(\"GPU not available. Running on CPU.\")\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "ram_mb = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "print(\"Current CPU RAM usage: {:.2f} MB\".format(ram_mb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040c060e",
   "metadata": {},
   "source": [
    "3D CNN: Saving the results for comparitive analyisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc3d = accuracy_score(all_labels_3d, all_preds_3d)\n",
    "p3d, r3d, f13d, _ = precision_recall_fscore_support(all_labels_3d, all_preds_3d, average=\"macro\")\n",
    "\n",
    "stats_3d = {\n",
    "    \"accuracy\": acc3d,\n",
    "    \"precision\": p3d,\n",
    "    \"recall\": r3d,\n",
    "    \"f1\": f13d,\n",
    "    \"inference_time\": inf_time_3d,\n",
    "    \"params\": total_params_3d,\n",
    "    \"model_size_mb\": file_size_mb_3d\n",
    "}\n",
    "\n",
    "with open(\"../results/stats_deep_learning/stats_3d.json\", \"w\") as f:\n",
    "    json.dump(stats_3d, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../results/stats_deep_learning/preds_2d.npy\", all_preds_2d)\n",
    "np.save(\"../results/stats_deep_learning/labels_2d.npy\", all_labels_2d)\n",
    "\n",
    "np.save(\"../results/stats_deep_learning/preds_3d.npy\", all_preds_3d)\n",
    "np.save(\"../results/stats_deep_learning/labels_3d.npy\", all_labels_3d)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
